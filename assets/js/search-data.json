{
  
    
        "post0": {
            "title": "FE Climate Change",
            "content": ". import pandas as pd import numpy as np from matplotlib import pyplot as plt import seaborn as sns . df = pd.read_csv(&quot;daily_serie_train.csv&quot;) . df.head() . date meantemp humidity wind_speed meanpressure . 0 2013-01-01 | 10.000000 | 84.500000 | 0.000000 | 1015.666667 | . 1 2013-01-02 | 7.400000 | 92.000000 | 2.980000 | 1017.800000 | . 2 2013-01-03 | 7.166667 | 87.000000 | 4.633333 | 1018.666667 | . 3 2013-01-04 | 8.666667 | 71.333333 | 1.233333 | 1017.166667 | . 4 2013-01-05 | 6.000000 | 86.833333 | 3.700000 | 1016.500000 | . df.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 1462 entries, 0 to 1461 Data columns (total 5 columns): # Column Non-Null Count Dtype -- -- 0 date 1462 non-null object 1 meantemp 1462 non-null float64 2 humidity 1462 non-null float64 3 wind_speed 1462 non-null float64 4 meanpressure 1462 non-null float64 dtypes: float64(4), object(1) memory usage: 57.2+ KB . df.isna().sum() . date 0 meantemp 0 humidity 0 wind_speed 0 meanpressure 0 dtype: int64 . df.date = pd.DatetimeIndex(df.date.values) df = df.set_index(&#39;date&#39;) df.head() . meantemp humidity wind_speed meanpressure . date . 2013-01-01 10.000000 | 84.500000 | 0.000000 | 1015.666667 | . 2013-01-02 7.400000 | 92.000000 | 2.980000 | 1017.800000 | . 2013-01-03 7.166667 | 87.000000 | 4.633333 | 1018.666667 | . 2013-01-04 8.666667 | 71.333333 | 1.233333 | 1017.166667 | . 2013-01-05 6.000000 | 86.833333 | 3.700000 | 1016.500000 | . df.describe() . meantemp humidity wind_speed meanpressure . count 1462.000000 | 1462.000000 | 1462.000000 | 1462.000000 | . mean 25.495521 | 60.771702 | 6.802209 | 1011.104548 | . std 7.348103 | 16.769652 | 4.561602 | 180.231668 | . min 6.000000 | 13.428571 | 0.000000 | -3.041667 | . 25% 18.857143 | 50.375000 | 3.475000 | 1001.580357 | . 50% 27.714286 | 62.625000 | 6.221667 | 1008.563492 | . 75% 31.305804 | 72.218750 | 9.238235 | 1014.944901 | . max 38.714286 | 100.000000 | 42.220000 | 7679.333333 | . Existem outliers, segundo boxplot anteriores do EDA, em: . humidade | pressão atmosférica | velocidade do vento | . def remove_outliers(df, col): q1 = df[col].quantile(0.25) q3 = df[col].quantile(0.75) iqr = q3-q1 df = df[(df[col]&gt;(q1-1.5*iqr)) &amp; (df[col]&lt;(q3+1.5*iqr))] return df . for col in [&quot;wind_speed&quot;,&quot;meanpressure&quot;]: df=remove_outliers(df, col) . df[[&#39;meanpressure&#39;]].boxplot() plt.show() . selected_features = [&#39;humidity&#39;, &#39;wind_speed&#39;] target = [&#39;meantemp&#39;] df = df[selected_features + target] . train = df[df.index&lt;&quot;2016-08-01&quot;] test = df[df.index&gt;=&quot;2016-08-01&quot;] . train[&#39;month&#39;] = train.index.month train = train.join(train.groupby(&#39;month&#39;).agg({&#39;meantemp&#39;:&#39;mean&#39;}).reset_index(), on=&#39;month&#39;, rsuffix=&#39;_month&#39;) . /usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy &#34;&#34;&#34;Entry point for launching an IPython kernel. . test[&#39;month&#39;] = test.index.month test = test.join(train.groupby(&#39;month&#39;).agg({&#39;meantemp&#39;:&#39;mean&#39;}).reset_index(), on=&#39;month&#39;, rsuffix=&#39;_month&#39;) . /usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy &#34;&#34;&#34;Entry point for launching an IPython kernel. . train.drop(columns=[&#39;month&#39;,&#39;month_month&#39;], inplace=True) test.drop(columns=[&#39;month&#39;,&#39;month_month&#39;], inplace=True) . from sklearn.preprocessing import MinMaxScaler . min_max = MinMaxScaler() . train = pd.DataFrame(data = min_max.fit_transform(train), columns=train.columns) . test = pd.DataFrame(data =min_max.transform(test), columns=test.columns) . train.head() . humidity wind_speed meantemp meantemp_month . 0 0.840372 | 0.000000 | 0.122271 | 0.140201 | . 1 0.929054 | 0.173888 | 0.042795 | 0.140201 | . 2 0.869932 | 0.270362 | 0.035662 | 0.140201 | . 3 0.684685 | 0.071967 | 0.081514 | 0.140201 | . 4 0.867962 | 0.215901 | 0.000000 | 0.140201 | . def remove_outliers(df, col): q1 = df[col].quantile(0,25) q3 = df[col].quantile(0,75) iqr = q3-q1 df[(df[col]&gt;(q1-1.5*iqr)) &amp; (df[col]&lt;(q3+1.5*iqr))] return df .",
            "url": "https://vinicius-l-r-matos.github.io/-Repositorio-DS/2022/01/30/_11_29_FE_Climate_Change_Delhi.html",
            "relUrl": "/2022/01/30/_11_29_FE_Climate_Change_Delhi.html",
            "date": " • Jan 30, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "EDA Climate Change Delhi",
            "content": ". Aqui vou trabalhar com um problema de séries temporais com o objetivo de realizar futuramente a previsão do clima médio nos próximos meses. . Desafio Clima Kaggel . EDA Realizada Anteriormente . EDA . import pandas as pd import numpy as np from matplotlib import pyplot as plt import seaborn as sns . df = pd.read_csv(&quot;daily_serie_train.csv&quot;) . df.head() . date meantemp humidity wind_speed meanpressure . 0 2013-01-01 | 10.000000 | 84.500000 | 0.000000 | 1015.666667 | . 1 2013-01-02 | 7.400000 | 92.000000 | 2.980000 | 1017.800000 | . 2 2013-01-03 | 7.166667 | 87.000000 | 4.633333 | 1018.666667 | . 3 2013-01-04 | 8.666667 | 71.333333 | 1.233333 | 1017.166667 | . 4 2013-01-05 | 6.000000 | 86.833333 | 3.700000 | 1016.500000 | . df.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 1462 entries, 0 to 1461 Data columns (total 5 columns): # Column Non-Null Count Dtype -- -- 0 date 1462 non-null object 1 meantemp 1462 non-null float64 2 humidity 1462 non-null float64 3 wind_speed 1462 non-null float64 4 meanpressure 1462 non-null float64 dtypes: float64(4), object(1) memory usage: 57.2+ KB . df.isna().sum() . date 0 meantemp 0 humidity 0 wind_speed 0 meanpressure 0 dtype: int64 . df.date = pd.DatetimeIndex(df.date.values) df = df.set_index(&#39;date&#39;) df.head() . meantemp humidity wind_speed meanpressure . date . 2013-01-01 10.000000 | 84.500000 | 0.000000 | 1015.666667 | . 2013-01-02 7.400000 | 92.000000 | 2.980000 | 1017.800000 | . 2013-01-03 7.166667 | 87.000000 | 4.633333 | 1018.666667 | . 2013-01-04 8.666667 | 71.333333 | 1.233333 | 1017.166667 | . 2013-01-05 6.000000 | 86.833333 | 3.700000 | 1016.500000 | . df.describe() . meantemp humidity wind_speed meanpressure . count 1462.000000 | 1462.000000 | 1462.000000 | 1462.000000 | . mean 25.495521 | 60.771702 | 6.802209 | 1011.104548 | . std 7.348103 | 16.769652 | 4.561602 | 180.231668 | . min 6.000000 | 13.428571 | 0.000000 | -3.041667 | . 25% 18.857143 | 50.375000 | 3.475000 | 1001.580357 | . 50% 27.714286 | 62.625000 | 6.221667 | 1008.563492 | . 75% 31.305804 | 72.218750 | 9.238235 | 1014.944901 | . max 38.714286 | 100.000000 | 42.220000 | 7679.333333 | . def remove_outliers(df, col): q1 = df[col].quantile(0.25) q3 = df[col].quantile(0.75) iqr = q3-q1 df = df[(df[col]&gt;(q1-1.5*iqr)) &amp; (df[col]&lt;(q3+1.5*iqr))] return df . for col in [&quot;wind_speed&quot;,&quot;meanpressure&quot;]: df=remove_outliers(df, col) . df[[&#39;meanpressure&#39;]].boxplot() plt.show() . selected_features = [&#39;humidity&#39;, &#39;wind_speed&#39;] target = [&#39;meantemp&#39;] df = df[selected_features + target] . train = df[df.index&lt;&quot;2016-08-01&quot;] test = df[df.index&gt;=&quot;2016-08-01&quot;] . train[&#39;month&#39;] = train.index.month train = train.join(train.groupby(&#39;month&#39;).agg({&#39;meantemp&#39;:&#39;mean&#39;}).reset_index(), on=&#39;month&#39;, rsuffix=&#39;_month&#39;) . /usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy &#34;&#34;&#34;Entry point for launching an IPython kernel. . test[&#39;month&#39;] = test.index.month test = test.join(train.groupby(&#39;month&#39;).agg({&#39;meantemp&#39;:&#39;mean&#39;}).reset_index(), on=&#39;month&#39;, rsuffix=&#39;_month&#39;) . /usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy &#34;&#34;&#34;Entry point for launching an IPython kernel. . train.drop(columns=[&#39;month&#39;,&#39;month_month&#39;], inplace=True) test.drop(columns=[&#39;month&#39;,&#39;month_month&#39;], inplace=True) . from sklearn.preprocessing import MinMaxScaler . min_max = MinMaxScaler() . train = pd.DataFrame(data = min_max.fit_transform(train), columns=train.columns) . test = pd.DataFrame(data =min_max.transform(test), columns=test.columns) . train.head() . humidity wind_speed meantemp meantemp_month . 0 0.840372 | 0.000000 | 0.122271 | 0.140025 | . 1 0.929054 | 0.167181 | 0.042795 | 0.140025 | . 2 0.869932 | 0.259935 | 0.035662 | 0.140025 | . 3 0.684685 | 0.069191 | 0.081514 | 0.140025 | . 4 0.867962 | 0.207574 | 0.000000 | 0.140025 | . for idx, column in enumerate(df.columns): fig, axs = plt.subplots(1,2) fig.set_size_inches(10,5) sns.boxplot(df[column], ax=axs[0]) sns.histplot(df[column], ax=axs[1], kde=True) plt.show() . /usr/local/lib/python3.7/dist-packages/seaborn/_decorators.py:43: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation. FutureWarning . /usr/local/lib/python3.7/dist-packages/seaborn/_decorators.py:43: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation. FutureWarning . /usr/local/lib/python3.7/dist-packages/seaborn/_decorators.py:43: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation. FutureWarning . df.resample(&quot;M&quot;).mean().head() . humidity wind_speed meantemp . date . 2013-01-31 73.028802 | 4.833913 | 12.074770 | . 2013-02-28 71.938563 | 7.474090 | 16.867560 | . 2013-03-31 57.686706 | 8.246956 | 22.996905 | . 2013-04-30 34.612103 | 8.046385 | 28.895119 | . 2013-05-31 28.938249 | 8.943452 | 33.776767 | . df.resample(&quot;M&quot;).mean().meantemp.plot() . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fdeb696b390&gt; . df.resample(&quot;H&quot;).ffill().head() . humidity wind_speed meantemp . date . 2013-01-01 00:00:00 84.5 | 0.0 | 10.0 | . 2013-01-01 01:00:00 84.5 | 0.0 | 10.0 | . 2013-01-01 02:00:00 84.5 | 0.0 | 10.0 | . 2013-01-01 03:00:00 84.5 | 0.0 | 10.0 | . 2013-01-01 04:00:00 84.5 | 0.0 | 10.0 | . df.resample(&quot;H&quot;).bfill() . humidity wind_speed meantemp . date . 2013-01-01 00:00:00 84.5 | 0.00 | 10.0 | . 2013-01-01 01:00:00 92.0 | 2.98 | 7.4 | . 2013-01-01 02:00:00 92.0 | 2.98 | 7.4 | . 2013-01-01 03:00:00 92.0 | 2.98 | 7.4 | . 2013-01-01 04:00:00 92.0 | 2.98 | 7.4 | . ... ... | ... | ... | . 2016-12-31 20:00:00 100.0 | 0.00 | 10.0 | . 2016-12-31 21:00:00 100.0 | 0.00 | 10.0 | . 2016-12-31 22:00:00 100.0 | 0.00 | 10.0 | . 2016-12-31 23:00:00 100.0 | 0.00 | 10.0 | . 2017-01-01 00:00:00 100.0 | 0.00 | 10.0 | . 35065 rows × 3 columns . df.plot(subplots=True, figsize=(15,20)) . array([&lt;matplotlib.axes._subplots.AxesSubplot object at 0x7fdeb689d690&gt;, &lt;matplotlib.axes._subplots.AxesSubplot object at 0x7fdeb684bbd0&gt;, &lt;matplotlib.axes._subplots.AxesSubplot object at 0x7fdeb6892c90&gt;], dtype=object) . df.corr()[&#39;humidity_lag&#39;]=df.humidity.shift() . df.dropna(inplace=True) . corr=df.corr() sns.heatmap(data=corr,annot=True) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fdeb6a10750&gt; . sns.pairplot(df) plt.title(&#39;Dispersão de pares&#39;) . Text(0.5, 1.0, &#39;Dispersão de pares&#39;) . from statsmodels.tsa.seasonal import seasonal_decompose result = seasonal_decompose(df.meantemp, freq=365) ax = result.plot() ax.set_size_inches(15,20) plt.show() . /usr/local/lib/python3.7/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead. import pandas.util.testing as tm . temp_log = np.log(df[&#39;meantemp&#39;]) . rolmean_log = temp_log.rolling(window=7).mean() rolstd = temp_log.rolling(window=7).std() fig, ax = plt.subplots(figsize=(15,10)) orig = plt.plot(temp_log, color = &#39;blue&#39;, label = &#39;Transformação Logarítmica&#39;) mean = plt.plot(rolmean_log, color = &#39;red&#39;, label = &#39;Média Móvel de Transformação&#39;) desvio = plt.plot(rolstd, color = &#39;black&#39;, label = &#39;Desvio Padrão Móvel&#39;) plt.legend(loc=&#39;best&#39;) plt.title(&#39;Estatísticas de rolagem - Log&#39;) ax.xaxis_date() fig.autofmt_xdate() plt.show(block=False) . from statsmodels.tsa.stattools import adfuller from statsmodels.tsa.stattools import acf, pacf print(&#39;Resultados: Dickey Fuller test: &#39;) adftest = adfuller(df[&#39;meantemp&#39;]) out = pd.Series(adftest[0:4], index=[&#39;Teste&#39;, &#39;p-valor&#39;, &#39;Lags&#39;, &#39;Numero de observações usadas&#39;]) for key,value in adftest[4].items(): out[&#39;Valor Crítico (%s): &#39; % key] = value print(out) . Resultados: Dickey Fuller test: Teste -2.031594 p-valor 0.272894 Lags 10.000000 Numero de observações usadas 1412.000000 Valor Crítico (1%): -3.434990 Valor Crítico (5%): -2.863589 Valor Crítico (10%): -2.567861 dtype: float64 . log_menos_media = temp_log - rolmean_log log_menos_media.dropna(inplace=True) . rolmean_log = log_menos_media.rolling(window=7).mean() rolstd = log_menos_media.rolling(window=7).std() fig, ax = plt.subplots(figsize=(15,10)) orig = plt.plot(log_menos_media, color = &#39;blue&#39;, label = &#39;Transformação Logarítmica&#39;) mean = plt.plot(rolmean_log, color = &#39;red&#39;, label = &#39;Média Móvel de Transformação&#39;) desvio = plt.plot(rolstd, color = &#39;black&#39;, label = &#39;Desvio Padrão Móvel&#39;) plt.legend(loc=&#39;best&#39;) plt.title(&#39;Estatísticas de rolagem - Log&#39;) ax.xaxis_date() fig.autofmt_xdate() plt.show(block=False) . print(&#39;Resultados: Dickey Fuller test: &#39;) adftest = adfuller(log_menos_media, autolag = &#39;AIC&#39;) dfoutput = pd.Series(adftest[0:4], index=[&#39;Teste&#39;, &#39;p-valor&#39;, &#39;#Lags&#39;, &#39;Numero de observações usadas&#39;]) for key,value in adftest[4].items(): dfoutput[&#39;Valor Crítico (%s): &#39; % key] = value print(dfoutput) . Resultados: Dickey Fuller test: Teste -1.354640e+01 p-valor 2.456760e-25 #Lags 8.000000e+00 Numero de observações usadas 1.408000e+03 Valor Crítico (1%): -3.435003e+00 Valor Crítico (5%): -2.863595e+00 Valor Crítico (10%): -2.567864e+00 dtype: float64 . from statsmodels.graphics.tsaplots import plot_acf, plot_pacf plot_acf(df.meantemp) plot_pacf(df.meantemp) plt.show() . Conclusão . A umidade pode ser uma variável. pois possui correlação fraca com a média de temperatura | .",
            "url": "https://vinicius-l-r-matos.github.io/-Repositorio-DS/2022/01/30/_11_29_EDA_Climate_Change_Delhi.html",
            "relUrl": "/2022/01/30/_11_29_EDA_Climate_Change_Delhi.html",
            "date": " • Jan 30, 2022"
        }
        
    
  
    
        ,"post2": {
            "title": "Limpeza Dataset Spotify via API",
            "content": ". Pr&#233; requisitos: . O resultado fica bem fiel. Para usar, necessita de: . Notebook do Colab aberto | Noções de Python | Conexão com a interet | Url da API que deseja usar | . 1 - Importando as bibliotecas: . Duas blibliotecas são necessárias aqui. Pandas e Numpy. . import pandas as pd import numpy as np . Aqui eu abri o csv obtido no Kaggle e rankeio de acordo com a popularidade das músicas. . df = pd.read_csv(&#39;spotify.csv&#39;, index_col=0) df.sort_values(&#39;song_popularity&#39;, ascending=False, inplace=True) df.head(5) . song_name song_popularity song_duration_ms acousticness danceability energy instrumentalness key liveness loudness audio_mode speechiness tempo time_signature audio_valence . 1757 Party In The U.S.A. | nao_sei | 0.8220000000000001kg | 0.519mol/L | 0.36 | 0.0 | 10 | 0.177 | -8.575 | 0 | 0.105 | 97.42 | 4 | 0.7 | NaN | . 7574 I Love It (&amp; Lil Pump) | 99 | 127946 | 0.0114kg | 0.901mol/L | 0.522 | 0.0 | 2.000 | 0.259 | -8.304 | 1 | 0.33 | 104.053 | 4 | 0.329 | . 11777 I Love It (&amp; Lil Pump) | 99 | 127946 | 0.0114kg | 0.901mol/L | 0.522 | 0.0 | 2.000 | 0.259 | -8.304 | 1 | 0.33 | 104.053 | 4 | 0.329 | . 4301 I Love It (&amp; Lil Pump) | 99 | 127946 | 0.0114kg | 0.901mol/L | 0.522 | 0.0 | 2.000 | 0.259 | -8.304 | 1 | 0.33 | 104.053 | 4 | 0.329 | . 14444 I Love It (&amp; Lil Pump) | 99 | 127946 | 0.0114kg | 0.901mol/L | 0.522 | 0.0 | 2.000 | 0.259 | -8.304 | 1 | 0.33 | 104.053 | 4 | 0.329 | . 2 - Inspe&#231;&#227;o Dataset: . print(df.shape) . (18835, 15) . df.head(5) . song_name song_popularity song_duration_ms acousticness danceability energy instrumentalness key liveness loudness audio_mode speechiness tempo time_signature audio_valence . 1757 Party In The U.S.A. | nao_sei | 0.8220000000000001kg | 0.519mol/L | 0.36 | 0.0 | 10 | 0.177 | -8.575 | 0 | 0.105 | 97.42 | 4 | 0.7 | NaN | . 7574 I Love It (&amp; Lil Pump) | 99 | 127946 | 0.0114kg | 0.901mol/L | 0.522 | 0.0 | 2.000 | 0.259 | -8.304 | 1 | 0.33 | 104.053 | 4 | 0.329 | . 11777 I Love It (&amp; Lil Pump) | 99 | 127946 | 0.0114kg | 0.901mol/L | 0.522 | 0.0 | 2.000 | 0.259 | -8.304 | 1 | 0.33 | 104.053 | 4 | 0.329 | . 4301 I Love It (&amp; Lil Pump) | 99 | 127946 | 0.0114kg | 0.901mol/L | 0.522 | 0.0 | 2.000 | 0.259 | -8.304 | 1 | 0.33 | 104.053 | 4 | 0.329 | . 14444 I Love It (&amp; Lil Pump) | 99 | 127946 | 0.0114kg | 0.901mol/L | 0.522 | 0.0 | 2.000 | 0.259 | -8.304 | 1 | 0.33 | 104.053 | 4 | 0.329 | . ## Doc - https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.info.html ## Verificando Tipo de Dados e Valores Não Nulos ## Inicialmente não possuimos dados nulo df.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; Int64Index: 18835 entries, 1757 to 9956 Data columns (total 15 columns): # Column Non-Null Count Dtype -- -- 0 song_name 18835 non-null object 1 song_popularity 18835 non-null object 2 song_duration_ms 18835 non-null object 3 acousticness 18835 non-null object 4 danceability 18835 non-null object 5 energy 18835 non-null object 6 instrumentalness 18835 non-null object 7 key 18835 non-null float64 8 liveness 18835 non-null object 9 loudness 18835 non-null object 10 audio_mode 18835 non-null object 11 speechiness 18835 non-null object 12 tempo 18835 non-null object 13 time_signature 18835 non-null object 14 audio_valence 18834 non-null float64 dtypes: float64(2), object(13) memory usage: 2.3+ MB . ## Doc - https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.describe.html?highlight=describe#pandas.DataFrame.describe ## Aqui observamos apenas duas colunas pois os formatos das outras esta como Object e assim ele não consegue calcular as agregações necessárias. df.describe() . key audio_valence . count 18835.000000 | 18834.000000 | . mean 5.288674 | 0.527958 | . std 3.614624 | 0.244635 | . min 0.000000 | 0.000000 | . 25% 2.000000 | 0.335000 | . 50% 5.000000 | 0.526500 | . 75% 8.000000 | 0.725000 | . max 11.000000 | 0.984000 | . 3 - Removendo duplicadas: . duplicados = df[df.duplicated()] print(duplicados) . song_name ... audio_valence 11777 I Love It (&amp; Lil Pump) ... 0.329 4301 I Love It (&amp; Lil Pump) ... 0.329 14444 I Love It (&amp; Lil Pump) ... 0.329 1229 I Love It (&amp; Lil Pump) ... 0.329 3443 I Love It (&amp; Lil Pump) ... 0.329 ... ... ... ... 14292 Get Dripped (feat. Playboi Carti) ... 0.904 7273 John Madden 2 ... 0.409 6514 THIS OLE BOY ... 0.764 14312 Transformer (feat. Nicki Minaj) ... 0.287 7275 Prince Charming ... 0.605 [3903 rows x 15 columns] . ## Exemplo de uso em um cenário onde vc pode ter diversos valores iguais mas a combinação que não pode se repetir é em duas chaves especificas. print(df[df.duplicated(subset=[&#39;song_name&#39;,&#39;audio_valence&#39;])]) . song_name ... audio_valence 11777 I Love It (&amp; Lil Pump) ... 0.3290 4301 I Love It (&amp; Lil Pump) ... 0.3290 14444 I Love It (&amp; Lil Pump) ... 0.3290 1229 I Love It (&amp; Lil Pump) ... 0.3290 3443 I Love It (&amp; Lil Pump) ... 0.3290 ... ... ... ... 7273 John Madden 2 ... 0.4090 6514 THIS OLE BOY ... 0.7640 14312 Transformer (feat. Nicki Minaj) ... 0.2870 7275 Prince Charming ... 0.6050 7939 99 Pace ... 0.0689 [4161 rows x 15 columns] . ## Doc - https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.drop_duplicates.html df.drop_duplicates(inplace=True) print(df.shape) df.head(5) . (14932, 15) . song_name song_popularity song_duration_ms acousticness danceability energy instrumentalness key liveness loudness audio_mode speechiness tempo time_signature audio_valence . 1757 Party In The U.S.A. | nao_sei | 0.8220000000000001kg | 0.519mol/L | 0.36 | 0.0 | 10 | 0.177 | -8.575 | 0 | 0.105 | 97.42 | 4 | 0.7 | NaN | . 7574 I Love It (&amp; Lil Pump) | 99 | 127946 | 0.0114kg | 0.901mol/L | 0.522 | 0.0 | 2.000 | 0.259 | -8.304 | 1 | 0.33 | 104.053 | 4 | 0.329 | . 17588 Taki Taki (with Selena Gomez, Ozuna &amp; Cardi B) | 98 | 212500 | 0.153kg | 0.841mol/L | 0.7979999999999999 | 3.33e-06 | 1.000 | 0.0618 | -4.206 | 0 | 0.229 | 95.948 | 4 | 0.591 | . 17394 Promises (with Sam Smith) | 98 | 213309 | 0.0119kg | 0.7809999999999999mol/L | 0.768 | 4.91e-06 | 11.000 | 0.325 | -5.9910000000000005 | 1 | 0.0394 | 123.07 | 4 | 0.486 | . 12665 Eastside (with Halsey &amp; Khalid) | 98 | 173799 | 0.555kg | 0.56mol/L | 0.68 | 0.0 | 6.000 | 0.116 | -7.648 | 0 | 0.321 | 89.391 | 4 | 0.319 | . 4 - Validando consist&#234;ncia: . Como vimos anteriormente temos campos que seriam númericos porém possuem texto e um texto que não condiz com o nome da coluna, aqui temos métricas de kg e mol/L . def remove_text (df, columns, text): for col in columns: df[col] = df[col].str.strip(text) . remove_text(df, [&#39;acousticness&#39;, &#39;danceability&#39;], &#39;mol/L&#39;) remove_text(df, [&#39;song_duration_ms&#39;, &#39;acousticness&#39;], &#39;kg&#39;) . df.head(5) . song_name song_popularity song_duration_ms acousticness danceability energy instrumentalness key liveness loudness audio_mode speechiness tempo time_signature audio_valence . 1757 Party In The U.S.A. | nao_sei | 0.8220000000000001 | 0.519 | 0.36 | 0.0 | 10 | 0.177 | -8.575 | 0 | 0.105 | 97.42 | 4 | 0.7 | NaN | . 7574 I Love It (&amp; Lil Pump) | 99 | 127946 | 0.0114 | 0.901 | 0.522 | 0.0 | 2.000 | 0.259 | -8.304 | 1 | 0.33 | 104.053 | 4 | 0.329 | . 17588 Taki Taki (with Selena Gomez, Ozuna &amp; Cardi B) | 98 | 212500 | 0.153 | 0.841 | 0.7979999999999999 | 3.33e-06 | 1.000 | 0.0618 | -4.206 | 0 | 0.229 | 95.948 | 4 | 0.591 | . 17394 Promises (with Sam Smith) | 98 | 213309 | 0.0119 | 0.7809999999999999 | 0.768 | 4.91e-06 | 11.000 | 0.325 | -5.9910000000000005 | 1 | 0.0394 | 123.07 | 4 | 0.486 | . 12665 Eastside (with Halsey &amp; Khalid) | 98 | 173799 | 0.555 | 0.56 | 0.68 | 0.0 | 6.000 | 0.116 | -7.648 | 0 | 0.321 | 89.391 | 4 | 0.319 | . 5 - Transforma&#231;&#245;es DataType: . ## Doc - https://pandas.pydata.org/docs/reference/api/pandas.Series.astype.html?highlight=astype#pandas.Series.astype def to_type(df, columns, type): for col in columns: print(col) df[col] = df[col].astype(type) numerical_cols = [&#39;song_duration_ms&#39;, &#39;acousticness&#39;, &#39;danceability&#39;, &#39;energy&#39;, &#39;instrumentalness&#39;, &#39;liveness&#39;, &#39;loudness&#39;, &#39;speechiness&#39;, &#39;tempo&#39;, &#39;audio_valence&#39;] categorical_cols = [&#39;song_popularity&#39;, &#39;key&#39;, &#39;audio_mode&#39;, &#39;time_signature&#39;] to_type(df, numerical_cols, &#39;float&#39;) to_type(df, categorical_cols, &#39;category&#39;) . song_duration_ms acousticness danceability energy . ValueError Traceback (most recent call last) &lt;ipython-input-15-8fa9f2911c24&gt; in &lt;module&gt;() 12 categorical_cols = [&#39;song_popularity&#39;, &#39;key&#39;, &#39;audio_mode&#39;, &#39;time_signature&#39;] 13 &gt; 14 to_type(df, numerical_cols, &#39;float&#39;) 15 to_type(df, categorical_cols, &#39;category&#39;) &lt;ipython-input-15-8fa9f2911c24&gt; in to_type(df, columns, type) 4 for col in columns: 5 print(col) -&gt; 6 df[col] = df[col].astype(type) 7 8 numerical_cols = [&#39;song_duration_ms&#39;, &#39;acousticness&#39;, &#39;danceability&#39;, /usr/local/lib/python3.7/dist-packages/pandas/core/generic.py in astype(self, dtype, copy, errors) 5546 else: 5547 # else, only a single dtype is given -&gt; 5548 new_data = self._mgr.astype(dtype=dtype, copy=copy, errors=errors,) 5549 return self._constructor(new_data).__finalize__(self, method=&#34;astype&#34;) 5550 /usr/local/lib/python3.7/dist-packages/pandas/core/internals/managers.py in astype(self, dtype, copy, errors) 602 self, dtype, copy: bool = False, errors: str = &#34;raise&#34; 603 ) -&gt; &#34;BlockManager&#34;: --&gt; 604 return self.apply(&#34;astype&#34;, dtype=dtype, copy=copy, errors=errors) 605 606 def convert( /usr/local/lib/python3.7/dist-packages/pandas/core/internals/managers.py in apply(self, f, align_keys, **kwargs) 407 applied = b.apply(f, **kwargs) 408 else: --&gt; 409 applied = getattr(b, f)(**kwargs) 410 result_blocks = _extend_blocks(applied, result_blocks) 411 /usr/local/lib/python3.7/dist-packages/pandas/core/internals/blocks.py in astype(self, dtype, copy, errors) 593 vals1d = values.ravel() 594 try: --&gt; 595 values = astype_nansafe(vals1d, dtype, copy=True) 596 except (ValueError, TypeError): 597 # e.g. astype_nansafe can fail on object-dtype of strings /usr/local/lib/python3.7/dist-packages/pandas/core/dtypes/cast.py in astype_nansafe(arr, dtype, copy, skipna) 995 if copy or is_object_dtype(arr) or is_object_dtype(dtype): 996 # Explicit copy, or required since NumPy can&#39;t view from / to object. --&gt; 997 return arr.astype(dtype, copy=True) 998 999 return arr.view(dtype) ValueError: could not convert string to float: &#39;nao_sei&#39; . df = df.replace([&#39;nao_sei&#39;], np.nan) . to_type(df, numerical_cols, &#39;float&#39;) to_type(df, categorical_cols, &#39;category&#39;) . song_duration_ms acousticness danceability energy instrumentalness liveness loudness speechiness . ValueError Traceback (most recent call last) &lt;ipython-input-17-60fe30a932ff&gt; in &lt;module&gt;() -&gt; 1 to_type(df, numerical_cols, &#39;float&#39;) 2 to_type(df, categorical_cols, &#39;category&#39;) &lt;ipython-input-15-8fa9f2911c24&gt; in to_type(df, columns, type) 4 for col in columns: 5 print(col) -&gt; 6 df[col] = df[col].astype(type) 7 8 numerical_cols = [&#39;song_duration_ms&#39;, &#39;acousticness&#39;, &#39;danceability&#39;, /usr/local/lib/python3.7/dist-packages/pandas/core/generic.py in astype(self, dtype, copy, errors) 5546 else: 5547 # else, only a single dtype is given -&gt; 5548 new_data = self._mgr.astype(dtype=dtype, copy=copy, errors=errors,) 5549 return self._constructor(new_data).__finalize__(self, method=&#34;astype&#34;) 5550 /usr/local/lib/python3.7/dist-packages/pandas/core/internals/managers.py in astype(self, dtype, copy, errors) 602 self, dtype, copy: bool = False, errors: str = &#34;raise&#34; 603 ) -&gt; &#34;BlockManager&#34;: --&gt; 604 return self.apply(&#34;astype&#34;, dtype=dtype, copy=copy, errors=errors) 605 606 def convert( /usr/local/lib/python3.7/dist-packages/pandas/core/internals/managers.py in apply(self, f, align_keys, **kwargs) 407 applied = b.apply(f, **kwargs) 408 else: --&gt; 409 applied = getattr(b, f)(**kwargs) 410 result_blocks = _extend_blocks(applied, result_blocks) 411 /usr/local/lib/python3.7/dist-packages/pandas/core/internals/blocks.py in astype(self, dtype, copy, errors) 593 vals1d = values.ravel() 594 try: --&gt; 595 values = astype_nansafe(vals1d, dtype, copy=True) 596 except (ValueError, TypeError): 597 # e.g. astype_nansafe can fail on object-dtype of strings /usr/local/lib/python3.7/dist-packages/pandas/core/dtypes/cast.py in astype_nansafe(arr, dtype, copy, skipna) 995 if copy or is_object_dtype(arr) or is_object_dtype(dtype): 996 # Explicit copy, or required since NumPy can&#39;t view from / to object. --&gt; 997 return arr.astype(dtype, copy=True) 998 999 return arr.view(dtype) ValueError: could not convert string to float: &#39;0.nao_sei&#39; . df[&#39;speechiness&#39;] = df[&#39;speechiness&#39;].replace([&#39;0.nao_sei&#39;], np.nan) . to_type(df, numerical_cols, &#39;float&#39;) to_type(df, categorical_cols, &#39;category&#39;) . song_duration_ms acousticness danceability energy instrumentalness liveness loudness speechiness tempo audio_valence song_popularity key audio_mode time_signature . df.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; Int64Index: 14932 entries, 1757 to 9956 Data columns (total 15 columns): # Column Non-Null Count Dtype -- -- 0 song_name 14932 non-null object 1 song_popularity 14931 non-null category 2 song_duration_ms 14932 non-null float64 3 acousticness 14932 non-null float64 4 danceability 14932 non-null float64 5 energy 14931 non-null float64 6 instrumentalness 14930 non-null float64 7 key 14932 non-null category 8 liveness 14928 non-null float64 9 loudness 14931 non-null float64 10 audio_mode 14931 non-null category 11 speechiness 14931 non-null float64 12 tempo 14931 non-null float64 13 time_signature 14931 non-null category 14 audio_valence 14931 non-null float64 dtypes: category(4), float64(10), object(1) memory usage: 1.4+ MB . ## Uma forma de validação é verificar a quantidade de elementos em cada uma das categorias. for col in categorical_cols: print(f&#39;{col}&#39;) print(df[col].value_counts().sort_values()) . song_popularity 99 1 100 1 98 4 97 4 96 5 ... 54 324 53 325 55 345 58 347 52 355 Name: song_popularity, Length: 101, dtype: int64 key 0.177 1 3.0 433 10.0 1045 8.0 1047 6.0 1048 4.0 1084 11.0 1223 5.0 1257 2.0 1399 9.0 1410 1.0 1596 7.0 1654 0.0 1735 Name: key, dtype: int64 audio_mode 0.105 1 0 5496 1 9434 Name: audio_mode, dtype: int64 time_signature 2800000000 1 0.7 1 0 3 1 67 5 195 3 684 4 13980 Name: time_signature, dtype: int64 . df[&#39;key&#39;] = df[&#39;key&#39;].replace([0.177], np.nan) df[&#39;audio_mode&#39;] = df[&#39;audio_mode&#39;].replace([&#39;0.105&#39;], np.nan) df[&#39;time_signature&#39;] = df[&#39;time_signature&#39;].replace([&#39;0.7&#39;, &#39;2800000000&#39;], np.nan) . A partir de agora, temos um dataset com o minimo de consistencia e sem valores duplicados . df.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; Int64Index: 14932 entries, 1757 to 9956 Data columns (total 15 columns): # Column Non-Null Count Dtype -- -- 0 song_name 14932 non-null object 1 song_popularity 14931 non-null category 2 song_duration_ms 14932 non-null float64 3 acousticness 14932 non-null float64 4 danceability 14932 non-null float64 5 energy 14931 non-null float64 6 instrumentalness 14930 non-null float64 7 key 14931 non-null category 8 liveness 14928 non-null float64 9 loudness 14931 non-null float64 10 audio_mode 14930 non-null category 11 speechiness 14931 non-null float64 12 tempo 14931 non-null float64 13 time_signature 14929 non-null category 14 audio_valence 14931 non-null float64 dtypes: category(4), float64(10), object(1) memory usage: 1.4+ MB . df.isna().sum() . song_name 0 song_popularity 1 song_duration_ms 0 acousticness 0 danceability 0 energy 1 instrumentalness 2 key 1 liveness 4 loudness 1 audio_mode 2 speechiness 1 tempo 1 time_signature 3 audio_valence 1 dtype: int64 . df[df[numerical_cols]&lt;0].count() . song_name 0 song_popularity 0 song_duration_ms 1 acousticness 0 danceability 0 energy 0 instrumentalness 0 key 0 liveness 1 loudness 14923 audio_mode 0 speechiness 0 tempo 0 time_signature 0 audio_valence 0 dtype: int64 . 6 - Remo&#231;&#227;o de Colunas: . Algumas colunas podem ser consideradas desnecessárias para nossa análise, isso porque elas não nos passam informações relevantes a respeito do que queremos descobrir, ou até mesmo porque possuem tantos dados faltantes que mais atrapalham do que ajudam. Nesses casos uma forma rápida e fácil de solucionar esse problema seria excluí-las. . Aqui eliminaremos apenas uma a nivel de experimentação. . ## Doc - https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.drop.html df.drop([&#39;liveness&#39;], axis=1) . song_name song_popularity song_duration_ms acousticness danceability energy instrumentalness key loudness audio_mode speechiness tempo time_signature audio_valence . 1757 Party In The U.S.A. | NaN | 0.822 | 0.51900 | 0.360 | 0.000 | 10.000000 | NaN | 0.000 | NaN | 97.4200 | 4.000 | NaN | NaN | . 7574 I Love It (&amp; Lil Pump) | 99 | 127946.000 | 0.01140 | 0.901 | 0.522 | 0.000000 | 2.0 | -8.304 | 1 | 0.3300 | 104.053 | 4 | 0.329 | . 17588 Taki Taki (with Selena Gomez, Ozuna &amp; Cardi B) | 98 | 212500.000 | 0.15300 | 0.841 | 0.798 | 0.000003 | 1.0 | -4.206 | 0 | 0.2290 | 95.948 | 4 | 0.591 | . 17394 Promises (with Sam Smith) | 98 | 213309.000 | 0.01190 | 0.781 | 0.768 | 0.000005 | 11.0 | -5.991 | 1 | 0.0394 | 123.070 | 4 | 0.486 | . 12665 Eastside (with Halsey &amp; Khalid) | 98 | 173799.000 | 0.55500 | 0.560 | 0.680 | 0.000000 | 6.0 | -7.648 | 0 | 0.3210 | 89.391 | 4 | 0.319 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 11278 María | 0 | 161986.000 | 0.90600 | 0.843 | 0.483 | 0.005230 | 3.0 | -14.776 | 1 | 0.0638 | 141.295 | 4 | 0.964 | . 12923 Unfuck The World | 0 | 250213.000 | 0.00142 | 0.574 | 0.831 | 0.010800 | 7.0 | -5.576 | 0 | 0.0325 | 101.988 | 4 | 0.518 | . 11282 Kimbya (feat. Manny Roman) | 0 | 261590.000 | 0.49600 | 0.418 | 0.958 | 0.058300 | 7.0 | -5.678 | 1 | 0.0728 | 123.639 | 4 | 0.676 | . 12905 Mad World | 0 | 174253.000 | 0.00002 | 0.298 | 0.931 | 0.404000 | 2.0 | -6.185 | 1 | 0.1300 | 135.970 | 4 | 0.404 | . 9956 All in My Feelings | 0 | 187123.000 | 0.51100 | 0.459 | 0.476 | 0.000000 | 2.0 | -5.277 | 1 | 0.0467 | 139.624 | 4 | 0.247 | . 14932 rows × 14 columns . df.drop(columns=[&#39;liveness&#39;], inplace=True) . KeyError Traceback (most recent call last) &lt;ipython-input-28-9052eefc4426&gt; in &lt;module&gt;() 1 ## Ou podemos deletar diretamente passando o parametro columns -&gt; 2 df.drop(columns=[&#39;liveness&#39;], inplace=True) /usr/local/lib/python3.7/dist-packages/pandas/core/frame.py in drop(self, labels, axis, index, columns, level, inplace, errors) 4172 level=level, 4173 inplace=inplace, -&gt; 4174 errors=errors, 4175 ) 4176 /usr/local/lib/python3.7/dist-packages/pandas/core/generic.py in drop(self, labels, axis, index, columns, level, inplace, errors) 3887 for axis, labels in axes.items(): 3888 if labels is not None: -&gt; 3889 obj = obj._drop_axis(labels, axis, level=level, errors=errors) 3890 3891 if inplace: /usr/local/lib/python3.7/dist-packages/pandas/core/generic.py in _drop_axis(self, labels, axis, level, errors) 3921 new_axis = axis.drop(labels, level=level, errors=errors) 3922 else: -&gt; 3923 new_axis = axis.drop(labels, errors=errors) 3924 result = self.reindex(**{axis_name: new_axis}) 3925 /usr/local/lib/python3.7/dist-packages/pandas/core/indexes/base.py in drop(self, labels, errors) 5285 if mask.any(): 5286 if errors != &#34;ignore&#34;: -&gt; 5287 raise KeyError(f&#34;{labels[mask]} not found in axis&#34;) 5288 indexer = indexer[~mask] 5289 return self.delete(indexer) KeyError: &#34;[&#39;liveness&#39;] not found in axis&#34; . 7 - Dados faltantes Missing Values: . Em algumas situações, podemos ter muitas informações incompletas no nosso df. Essas informações faltantes podem prejudicar nossa análise e outras etapas que dependem dela e do pré-processamento, portanto, precisamos removê-los ou substituir esses valores por outros. O fluxo a seguir pode auxiliar na decisão e trazer sugestões de como tratar cada caso. . . Para dados que não são séries temporais, nossa primeira opção é substitui-los pela média da coluna, entretanto, às vezes, a média pode ter sido afetada pelos valores destoantes da coluna (outliers), então podemos substituir também pela moda ou mediana. . Podemos fazer isso com a função .fillna que preenche todos os campos com dados ausentes. Vamos criar alguns loops como exemplo. O primeiro passa por algumas colunas e substitui os valores faltantes pela moda: . df.isna().sum() . song_name 0 song_popularity 1 song_duration_ms 0 acousticness 0 danceability 0 energy 1 instrumentalness 2 key 1 loudness 1 audio_mode 2 speechiness 1 tempo 1 time_signature 3 audio_valence 1 dtype: int64 . ## Doc .fillna - https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.fillna.html for column in [&#39;acousticness&#39;, &#39;speechiness&#39;]: df[column].fillna(df[column].mode()[0], inplace=True) . for column in [&#39;song_duration_ms&#39;, &#39;danceability&#39;, &#39;energy&#39;, &#39;loudness&#39;, &#39;audio_valence&#39;]: df[column].fillna(df[column].median(), inplace=True) . df.isna().sum() . song_name 0 song_popularity 1 song_duration_ms 0 acousticness 0 danceability 0 energy 0 instrumentalness 2 key 1 loudness 0 audio_mode 2 speechiness 0 tempo 1 time_signature 3 audio_valence 0 dtype: int64 . ## Doc - https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.dropna.html?highlight=dropna#pandas.DataFrame.dropna df.dropna(inplace=True) . df.isna().sum() . song_name 0 song_popularity 0 song_duration_ms 0 acousticness 0 danceability 0 energy 0 instrumentalness 0 key 0 loudness 0 audio_mode 0 speechiness 0 tempo 0 time_signature 0 audio_valence 0 dtype: int64 . Conclus&#227;o . Ao final temos o dataset pronto para a análise exploratória, aqui ainda não tratamos outliers pois dependendo do cenário podemos fazer uso deles. . df.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; Int64Index: 14925 entries, 7574 to 9956 Data columns (total 14 columns): # Column Non-Null Count Dtype -- -- 0 song_name 14925 non-null object 1 song_popularity 14925 non-null category 2 song_duration_ms 14925 non-null float64 3 acousticness 14925 non-null float64 4 danceability 14925 non-null float64 5 energy 14925 non-null float64 6 instrumentalness 14925 non-null float64 7 key 14925 non-null category 8 loudness 14925 non-null float64 9 audio_mode 14925 non-null category 10 speechiness 14925 non-null float64 11 tempo 14925 non-null float64 12 time_signature 14925 non-null category 13 audio_valence 14925 non-null float64 dtypes: category(4), float64(9), object(1) memory usage: 1.3+ MB . df.head() . song_name song_popularity song_duration_ms acousticness danceability energy instrumentalness key loudness audio_mode speechiness tempo time_signature audio_valence . 7574 I Love It (&amp; Lil Pump) | 99 | 127946.0 | 0.0114 | 0.901 | 0.522 | 0.000000 | 2.0 | -8.304 | 1 | 0.3300 | 104.053 | 4 | 0.329 | . 17588 Taki Taki (with Selena Gomez, Ozuna &amp; Cardi B) | 98 | 212500.0 | 0.1530 | 0.841 | 0.798 | 0.000003 | 1.0 | -4.206 | 0 | 0.2290 | 95.948 | 4 | 0.591 | . 17394 Promises (with Sam Smith) | 98 | 213309.0 | 0.0119 | 0.781 | 0.768 | 0.000005 | 11.0 | -5.991 | 1 | 0.0394 | 123.070 | 4 | 0.486 | . 12665 Eastside (with Halsey &amp; Khalid) | 98 | 173799.0 | 0.5550 | 0.560 | 0.680 | 0.000000 | 6.0 | -7.648 | 0 | 0.3210 | 89.391 | 4 | 0.319 | . 17618 In My Feelings | 98 | 217925.0 | 0.0589 | 0.835 | 0.626 | 0.000060 | 1.0 | -5.833 | 1 | 0.1250 | 91.030 | 4 | 0.350 | .",
            "url": "https://vinicius-l-r-matos.github.io/-Repositorio-DS/2022/01/30/_11_20_Limpeza_Dataset_Spotify_via_API.html",
            "relUrl": "/2022/01/30/_11_20_Limpeza_Dataset_Spotify_via_API.html",
            "date": " • Jan 30, 2022"
        }
        
    
  
    
        ,"post3": {
            "title": "EDA - Venda de jogos",
            "content": ". Uma contribuição para análise de venda de jogos . Abaixo Documentações Libs Gráficos: . Matplotlib | Seaborn | . Opções de EDA: . Predict Sales . PUGB Finish Predict . Predict Price . Netflix Dataset . Predict Imdb Rate . Desafio Escolhido . Video Game Sales . Poss&#237;veis Perguntas . Qual o jogo mais vendido por região/Genero/Plataforma ? OK | Jogo infantil vende mais do que adultos/Cultura do Pais também influência ? - Procurar base para join com classificação | Jogos exclusivos vendem mais ? OK | Concorrencia entre exclusivos (Principais Fabricantes Video Game)? | Produtora que mais vende e mais jogos ? Venda por jogo ? Ok | Será que a os NA indicam o comportamento do resto do mundo ? OK | Há anos com mais vendas de jogos ? OK | Genero por Região ? OK | . !pip install -U seaborn . Requirement already satisfied: seaborn in /usr/local/lib/python3.7/dist-packages (0.11.2) Requirement already satisfied: scipy&gt;=1.0 in /usr/local/lib/python3.7/dist-packages (from seaborn) (1.4.1) Requirement already satisfied: numpy&gt;=1.15 in /usr/local/lib/python3.7/dist-packages (from seaborn) (1.19.5) Requirement already satisfied: pandas&gt;=0.23 in /usr/local/lib/python3.7/dist-packages (from seaborn) (1.1.5) Requirement already satisfied: matplotlib&gt;=2.2 in /usr/local/lib/python3.7/dist-packages (from seaborn) (3.2.2) Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,&gt;=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib&gt;=2.2-&gt;seaborn) (3.0.6) Requirement already satisfied: kiwisolver&gt;=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib&gt;=2.2-&gt;seaborn) (1.3.2) Requirement already satisfied: python-dateutil&gt;=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib&gt;=2.2-&gt;seaborn) (2.8.2) Requirement already satisfied: cycler&gt;=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib&gt;=2.2-&gt;seaborn) (0.11.0) Requirement already satisfied: pytz&gt;=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas&gt;=0.23-&gt;seaborn) (2018.9) Requirement already satisfied: six&gt;=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil&gt;=2.1-&gt;matplotlib&gt;=2.2-&gt;seaborn) (1.15.0) . import seaborn as sns import matplotlib.pyplot as plt import pandas as pd import numpy as np . %matplotlib inline . from scipy import stats . df = pd.read_csv(&#39;vgsales.csv&#39;) . . df.shape . (16598, 11) . df.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 16598 entries, 0 to 16597 Data columns (total 11 columns): # Column Non-Null Count Dtype -- -- 0 Rank 16598 non-null int64 1 Name 16598 non-null object 2 Platform 16598 non-null object 3 Year 16327 non-null float64 4 Genre 16598 non-null object 5 Publisher 16540 non-null object 6 NA_Sales 16598 non-null float64 7 EU_Sales 16598 non-null float64 8 JP_Sales 16598 non-null float64 9 Other_Sales 16598 non-null float64 10 Global_Sales 16598 non-null float64 dtypes: float64(6), int64(1), object(4) memory usage: 1.4+ MB . df.head() . Rank Name Platform Year Genre Publisher NA_Sales EU_Sales JP_Sales Other_Sales Global_Sales . 0 1 | Wii Sports | Wii | 2006.0 | Sports | Nintendo | 41.49 | 29.02 | 3.77 | 8.46 | 82.74 | . 1 2 | Super Mario Bros. | NES | 1985.0 | Platform | Nintendo | 29.08 | 3.58 | 6.81 | 0.77 | 40.24 | . 2 3 | Mario Kart Wii | Wii | 2008.0 | Racing | Nintendo | 15.85 | 12.88 | 3.79 | 3.31 | 35.82 | . 3 4 | Wii Sports Resort | Wii | 2009.0 | Sports | Nintendo | 15.75 | 11.01 | 3.28 | 2.96 | 33.00 | . 4 5 | Pokemon Red/Pokemon Blue | GB | 1996.0 | Role-Playing | Nintendo | 11.27 | 8.89 | 10.22 | 1.00 | 31.37 | . df.isna().sum()/df.shape[0] . Rank 0.000000 Name 0.000000 Platform 0.000000 Year 0.016327 Genre 0.000000 Publisher 0.003494 NA_Sales 0.000000 EU_Sales 0.000000 JP_Sales 0.000000 Other_Sales 0.000000 Global_Sales 0.000000 dtype: float64 . df.columns.str.lower() . Index([&#39;rank&#39;, &#39;name&#39;, &#39;platform&#39;, &#39;year&#39;, &#39;genre&#39;, &#39;publisher&#39;, &#39;na_sales&#39;, &#39;eu_sales&#39;, &#39;jp_sales&#39;, &#39;other_sales&#39;, &#39;global_sales&#39;], dtype=&#39;object&#39;) . columns_renamed = { &#39;Rank&#39;: &#39;rank&#39;, &#39;Name&#39;: &#39;name&#39;, &#39;Platform&#39;: &#39;platform&#39;, &#39;Year&#39;: &#39;year&#39;, &#39;Genre&#39;: &#39;genre&#39;, &#39;Publisher&#39;: &#39;publisher&#39;, &#39;NA_Sales&#39;: &#39;na_sales&#39;, &#39;EU_Sales&#39;: &#39;eu_sales&#39;, &#39;JP_Sales&#39;: &#39;jp_sales&#39;, &#39;Other_Sales&#39;: &#39;other_sales&#39;, &#39;Global_Sales&#39;: &#39;global_sales&#39; } df.rename(columns=columns_renamed, inplace=True) . df[df.year.isna()] . rank name platform year genre publisher na_sales eu_sales jp_sales other_sales global_sales . 179 180 | Madden NFL 2004 | PS2 | NaN | Sports | Electronic Arts | 4.26 | 0.26 | 0.01 | 0.71 | 5.23 | . 377 378 | FIFA Soccer 2004 | PS2 | NaN | Sports | Electronic Arts | 0.59 | 2.36 | 0.04 | 0.51 | 3.49 | . 431 432 | LEGO Batman: The Videogame | Wii | NaN | Action | Warner Bros. Interactive Entertainment | 1.86 | 1.02 | 0.00 | 0.29 | 3.17 | . 470 471 | wwe Smackdown vs. Raw 2006 | PS2 | NaN | Fighting | NaN | 1.57 | 1.02 | 0.00 | 0.41 | 3.00 | . 607 608 | Space Invaders | 2600 | NaN | Shooter | Atari | 2.36 | 0.14 | 0.00 | 0.03 | 2.53 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 16307 16310 | Freaky Flyers | GC | NaN | Racing | Unknown | 0.01 | 0.00 | 0.00 | 0.00 | 0.01 | . 16327 16330 | Inversion | PC | NaN | Shooter | Namco Bandai Games | 0.01 | 0.00 | 0.00 | 0.00 | 0.01 | . 16366 16369 | Hakuouki: Shinsengumi Kitan | PS3 | NaN | Adventure | Unknown | 0.01 | 0.00 | 0.00 | 0.00 | 0.01 | . 16427 16430 | Virtua Quest | GC | NaN | Role-Playing | Unknown | 0.01 | 0.00 | 0.00 | 0.00 | 0.01 | . 16493 16496 | The Smurfs | 3DS | NaN | Action | Unknown | 0.00 | 0.01 | 0.00 | 0.00 | 0.01 | . 271 rows × 11 columns . df[df.publisher.isna()] . rank name platform year genre publisher na_sales eu_sales jp_sales other_sales global_sales . 470 471 | wwe Smackdown vs. Raw 2006 | PS2 | NaN | Fighting | NaN | 1.57 | 1.02 | 0.00 | 0.41 | 3.00 | . 1303 1305 | Triple Play 99 | PS | NaN | Sports | NaN | 0.81 | 0.55 | 0.00 | 0.10 | 1.46 | . 1662 1664 | Shrek / Shrek 2 2-in-1 Gameboy Advance Video | GBA | 2007.0 | Misc | NaN | 0.87 | 0.32 | 0.00 | 0.02 | 1.21 | . 2222 2224 | Bentley&#39;s Hackpack | GBA | 2005.0 | Misc | NaN | 0.67 | 0.25 | 0.00 | 0.02 | 0.93 | . 3159 3161 | Nicktoons Collection: Game Boy Advance Video V... | GBA | 2004.0 | Misc | NaN | 0.46 | 0.17 | 0.00 | 0.01 | 0.64 | . 3166 3168 | SpongeBob SquarePants: Game Boy Advance Video ... | GBA | 2004.0 | Misc | NaN | 0.46 | 0.17 | 0.00 | 0.01 | 0.64 | . 3766 3768 | SpongeBob SquarePants: Game Boy Advance Video ... | GBA | 2004.0 | Misc | NaN | 0.38 | 0.14 | 0.00 | 0.01 | 0.53 | . 4145 4147 | Sonic the Hedgehog | PS3 | NaN | Platform | NaN | 0.00 | 0.48 | 0.00 | 0.00 | 0.48 | . 4526 4528 | The Fairly Odd Parents: Game Boy Advance Video... | GBA | 2004.0 | Misc | NaN | 0.31 | 0.11 | 0.00 | 0.01 | 0.43 | . 4635 4637 | The Fairly Odd Parents: Game Boy Advance Video... | GBA | 2004.0 | Misc | NaN | 0.30 | 0.11 | 0.00 | 0.01 | 0.42 | . 5302 5304 | Dragon Ball Z: Budokai Tenkaichi 2 (JP sales) | Wii | NaN | Action | NaN | 0.15 | 0.05 | 0.14 | 0.01 | 0.35 | . 5647 5649 | Cartoon Network Collection: Game Boy Advance V... | GBA | 2005.0 | Misc | NaN | 0.23 | 0.08 | 0.00 | 0.01 | 0.32 | . 6272 6274 | The Legend of Zelda: The Minish Cap(weekly JP ... | GBA | NaN | Action | NaN | 0.00 | 0.00 | 0.27 | 0.01 | 0.27 | . 6437 6439 | Sonic X: Game Boy Advance Video Volume 1 | GBA | 2004.0 | Misc | NaN | 0.19 | 0.07 | 0.00 | 0.00 | 0.27 | . 6562 6564 | Dora the Explorer: Game Boy Advance Video Volu... | GBA | 2004.0 | Misc | NaN | 0.18 | 0.07 | 0.00 | 0.00 | 0.26 | . 6648 6650 | Cartoon Network Collection: Game Boy Advance V... | GBA | 2004.0 | Misc | NaN | 0.18 | 0.07 | 0.00 | 0.00 | 0.25 | . 6849 6851 | All Grown Up!: Game Boy Advance Video Volume 1 | GBA | 2004.0 | Misc | NaN | 0.17 | 0.06 | 0.00 | 0.00 | 0.24 | . 7208 7210 | Nicktoons Collection: Game Boy Advance Video V... | GBA | 2004.0 | Misc | NaN | 0.16 | 0.06 | 0.00 | 0.00 | 0.22 | . 7351 7353 | Yu Yu Hakusho: Dark Tournament | PS2 | NaN | Fighting | NaN | 0.10 | 0.08 | 0.00 | 0.03 | 0.21 | . 7470 7472 | SpongeBob SquarePants: Game Boy Advance Video ... | GBA | 2004.0 | Misc | NaN | 0.15 | 0.05 | 0.00 | 0.00 | 0.21 | . 7953 7955 | Thomas the Tank Engine &amp; Friends | GBA | 2004.0 | Adventure | NaN | 0.13 | 0.05 | 0.00 | 0.00 | 0.19 | . 8330 8332 | Dragon Ball GT: Game Boy Advance Video Volume 1 | GBA | 2004.0 | Misc | NaN | 0.12 | 0.05 | 0.00 | 0.00 | 0.17 | . 8341 8343 | Codename: Kids Next Door: Game Boy Advance Vid... | GBA | 2004.0 | Misc | NaN | 0.12 | 0.05 | 0.00 | 0.00 | 0.17 | . 8368 8370 | Teenage Mutant Ninja Turtles: Game Boy Advance... | GBA | 2004.0 | Misc | NaN | 0.12 | 0.04 | 0.00 | 0.00 | 0.17 | . 8503 8505 | Stronghold 3 | PC | 2011.0 | Strategy | NaN | 0.06 | 0.10 | 0.00 | 0.00 | 0.16 | . 8770 8772 | Cartoon Network Collection: Game Boy Advance V... | GBA | 2005.0 | Misc | NaN | 0.11 | 0.04 | 0.00 | 0.00 | 0.15 | . 8848 8850 | Pokémon: Johto Photo Finish: Game Boy Advance ... | GBA | 2004.0 | Misc | NaN | 0.11 | 0.04 | 0.00 | 0.00 | 0.15 | . 8896 8898 | Strawberry Shortcake: Game Boy Advance Video V... | GBA | 2004.0 | Misc | NaN | 0.11 | 0.04 | 0.00 | 0.00 | 0.15 | . 9517 9519 | Farming Simulator 2011 | PC | 2010.0 | Simulation | NaN | 0.00 | 0.13 | 0.00 | 0.00 | 0.13 | . 9749 9751 | Super Robot Wars OG Saga: Masou Kishin II - Re... | PSP | NaN | Strategy | NaN | 0.00 | 0.00 | 0.12 | 0.00 | 0.12 | . 10382 10384 | Disney Channel Collection Vol. 1 | GBA | 2004.0 | Misc | NaN | 0.08 | 0.03 | 0.00 | 0.00 | 0.11 | . 10494 10496 | Atsumare! Power Pro Kun no DS Koushien | DS | NaN | Sports | NaN | 0.00 | 0.00 | 0.10 | 0.00 | 0.10 | . 11076 11078 | Action Man-Operation Extreme | PS | NaN | Action | NaN | 0.05 | 0.03 | 0.00 | 0.01 | 0.09 | . 11526 11528 | Cartoon Network Collection: Game Boy Advance V... | GBA | 2004.0 | Misc | NaN | 0.06 | 0.02 | 0.00 | 0.00 | 0.08 | . 12487 12489 | Chou Soujuu Mecha MG | DS | NaN | Simulation | NaN | 0.00 | 0.00 | 0.06 | 0.00 | 0.06 | . 12517 12519 | Prinny: Can I Really Be The Hero? (US sales) | PSP | NaN | Action | NaN | 0.06 | 0.00 | 0.00 | 0.00 | 0.06 | . 13278 13280 | Monster Hunter Frontier Online | PS3 | NaN | Role-Playing | NaN | 0.00 | 0.00 | 0.05 | 0.00 | 0.05 | . 13672 13674 | B.L.U.E.: Legend of Water | PS | NaN | Adventure | NaN | 0.00 | 0.00 | 0.04 | 0.00 | 0.04 | . 13962 13964 | World of Tanks | X360 | NaN | Shooter | NaN | 0.00 | 0.03 | 0.00 | 0.00 | 0.04 | . 14087 14089 | Housekeeping | DS | NaN | Action | NaN | 0.00 | 0.00 | 0.04 | 0.00 | 0.04 | . 14296 14299 | Bikkuriman Daijiten | DS | NaN | Misc | NaN | 0.00 | 0.00 | 0.03 | 0.00 | 0.03 | . 14311 14314 | Silverlicious | DS | 2012.0 | Action | NaN | 0.03 | 0.00 | 0.00 | 0.00 | 0.03 | . 14698 14701 | UK Truck Simulator | PC | 2010.0 | Simulation | NaN | 0.00 | 0.03 | 0.00 | 0.00 | 0.03 | . 14942 14945 | Umineko no Naku Koro ni San: Shinjitsu to Gens... | PS3 | NaN | Adventure | NaN | 0.00 | 0.00 | 0.02 | 0.00 | 0.02 | . 15056 15059 | Xia-Xia | DS | 2012.0 | Platform | NaN | 0.00 | 0.02 | 0.00 | 0.00 | 0.02 | . 15261 15264 | Mario Tennis | 3DS | NaN | Sports | NaN | 0.00 | 0.00 | 0.02 | 0.00 | 0.02 | . 15325 15328 | Nicktoons Collection: Game Boy Advance Video V... | GBA | 2005.0 | Misc | NaN | 0.01 | 0.01 | 0.00 | 0.00 | 0.02 | . 15353 15356 | Demolition Company: Gold Edition | PC | 2011.0 | Simulation | NaN | 0.00 | 0.02 | 0.00 | 0.00 | 0.02 | . 15788 15791 | Moshi, Kono Sekai ni Kami-sama ga Iru to suru ... | PSV | 2016.0 | Adventure | NaN | 0.00 | 0.00 | 0.02 | 0.00 | 0.02 | . 15915 15918 | Dream Dancer | DS | NaN | Misc | NaN | 0.01 | 0.00 | 0.00 | 0.00 | 0.02 | . 16191 16194 | Homeworld Remastered Collection | PC | NaN | Strategy | NaN | 0.00 | 0.01 | 0.00 | 0.00 | 0.01 | . 16198 16201 | AKB1/48: Idol to Guam de Koishitara... | X360 | NaN | Misc | NaN | 0.00 | 0.00 | 0.01 | 0.00 | 0.01 | . 16208 16211 | Super Robot Monkey Team: Game Boy Advance Vide... | GBA | 2005.0 | Misc | NaN | 0.01 | 0.00 | 0.00 | 0.00 | 0.01 | . 16229 16232 | Brothers in Arms: Furious 4 | X360 | NaN | Shooter | NaN | 0.01 | 0.00 | 0.00 | 0.00 | 0.01 | . 16367 16370 | Dance with Devils | PSV | 2016.0 | Action | NaN | 0.00 | 0.00 | 0.01 | 0.00 | 0.01 | . 16494 16497 | Legends of Oz: Dorothy&#39;s Return | 3DS | 2014.0 | Puzzle | NaN | 0.00 | 0.01 | 0.00 | 0.00 | 0.01 | . 16543 16546 | Driving Simulator 2011 | PC | 2011.0 | Racing | NaN | 0.00 | 0.01 | 0.00 | 0.00 | 0.01 | . 16553 16556 | Bound By Flame | X360 | 2014.0 | Role-Playing | NaN | 0.00 | 0.01 | 0.00 | 0.00 | 0.01 | . df.dropna(inplace=True) . df.year =df.year.astype(int) . df.shape . (16291, 11) . df[df.duplicated()].count() . rank 0 name 0 platform 0 year 0 genre 0 publisher 0 na_sales 0 eu_sales 0 jp_sales 0 other_sales 0 global_sales 0 dtype: int64 . df.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; Int64Index: 16291 entries, 0 to 16597 Data columns (total 11 columns): # Column Non-Null Count Dtype -- -- 0 rank 16291 non-null int64 1 name 16291 non-null object 2 platform 16291 non-null object 3 year 16291 non-null int64 4 genre 16291 non-null object 5 publisher 16291 non-null object 6 na_sales 16291 non-null float64 7 eu_sales 16291 non-null float64 8 jp_sales 16291 non-null float64 9 other_sales 16291 non-null float64 10 global_sales 16291 non-null float64 dtypes: float64(5), int64(2), object(4) memory usage: 1.5+ MB . df.head() . rank name platform year genre publisher na_sales eu_sales jp_sales other_sales global_sales . 0 1 | Wii Sports | Wii | 2006 | Sports | Nintendo | 41.49 | 29.02 | 3.77 | 8.46 | 82.74 | . 1 2 | Super Mario Bros. | NES | 1985 | Platform | Nintendo | 29.08 | 3.58 | 6.81 | 0.77 | 40.24 | . 2 3 | Mario Kart Wii | Wii | 2008 | Racing | Nintendo | 15.85 | 12.88 | 3.79 | 3.31 | 35.82 | . 3 4 | Wii Sports Resort | Wii | 2009 | Sports | Nintendo | 15.75 | 11.01 | 3.28 | 2.96 | 33.00 | . 4 5 | Pokemon Red/Pokemon Blue | GB | 1996 | Role-Playing | Nintendo | 11.27 | 8.89 | 10.22 | 1.00 | 31.37 | . . df.describe() . rank year na_sales eu_sales jp_sales other_sales global_sales . count 16291.000000 | 16291.000000 | 16291.000000 | 16291.000000 | 16291.000000 | 16291.000000 | 16291.000000 | . mean 8290.190228 | 2006.405561 | 0.265647 | 0.147731 | 0.078833 | 0.048426 | 0.540910 | . std 4792.654450 | 5.832412 | 0.822432 | 0.509303 | 0.311879 | 0.190083 | 1.567345 | . min 1.000000 | 1980.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.010000 | . 25% 4132.500000 | 2003.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.060000 | . 50% 8292.000000 | 2007.000000 | 0.080000 | 0.020000 | 0.000000 | 0.010000 | 0.170000 | . 75% 12439.500000 | 2010.000000 | 0.240000 | 0.110000 | 0.040000 | 0.040000 | 0.480000 | . max 16600.000000 | 2020.000000 | 41.490000 | 29.020000 | 10.220000 | 10.570000 | 82.740000 | . df_top_games = df[[&#39;name&#39;, &#39;na_sales&#39;, &#39;eu_sales&#39;, &#39;jp_sales&#39;, &#39;other_sales&#39;]] . for col in [&#39;na_sales&#39;, &#39;eu_sales&#39;, &#39;jp_sales&#39;, &#39;other_sales&#39;]: print(col) print(df_top_games[df_top_games[col]==df_top_games[col].max()]) . na_sales name na_sales eu_sales jp_sales other_sales 0 Wii Sports 41.49 29.02 3.77 8.46 eu_sales name na_sales eu_sales jp_sales other_sales 0 Wii Sports 41.49 29.02 3.77 8.46 jp_sales name na_sales eu_sales jp_sales other_sales 4 Pokemon Red/Pokemon Blue 11.27 8.89 10.22 1.0 other_sales name na_sales eu_sales jp_sales other_sales 17 Grand Theft Auto: San Andreas 9.43 0.4 0.41 10.57 . O Jogo mais vendido nas regiões foram Wii Sports, Pokemon, GTA . for col in [&#39;na_sales&#39;, &#39;eu_sales&#39;, &#39;jp_sales&#39;, &#39;other_sales&#39;]: df_plot = df_top_games.sort_values(by=col, ascending=False).head(5) df_plot[[&#39;name&#39;,col]].set_index(&#39;name&#39;).plot.bar(rot=90) plt.title(f&#39;Sales in {col}&#39;) . df.head() . rank name platform year genre publisher na_sales eu_sales jp_sales other_sales global_sales . 0 1 | Wii Sports | Wii | 2006 | Sports | Nintendo | 41.49 | 29.02 | 3.77 | 8.46 | 82.74 | . 1 2 | Super Mario Bros. | NES | 1985 | Platform | Nintendo | 29.08 | 3.58 | 6.81 | 0.77 | 40.24 | . 2 3 | Mario Kart Wii | Wii | 2008 | Racing | Nintendo | 15.85 | 12.88 | 3.79 | 3.31 | 35.82 | . 3 4 | Wii Sports Resort | Wii | 2009 | Sports | Nintendo | 15.75 | 11.01 | 3.28 | 2.96 | 33.00 | . 4 5 | Pokemon Red/Pokemon Blue | GB | 1996 | Role-Playing | Nintendo | 11.27 | 8.89 | 10.22 | 1.00 | 31.37 | . df_genre = df[[&#39;name&#39;,&#39;genre&#39;, &#39;global_sales&#39;]] . df_genre.groupby(&#39;genre&#39;).agg({&#39;name&#39;:&#39;first&#39;, &#39;global_sales&#39;:&#39;max&#39;}) . name global_sales . genre . Action Grand Theft Auto V | 21.40 | . Adventure Super Mario Land 2: 6 Golden Coins | 11.18 | . Fighting Super Smash Bros. Brawl | 13.04 | . Misc Wii Play | 29.02 | . Platform Super Mario Bros. | 40.24 | . Puzzle Tetris | 30.26 | . Racing Mario Kart Wii | 35.82 | . Role-Playing Pokemon Red/Pokemon Blue | 31.37 | . Shooter Duck Hunt | 28.31 | . Simulation Nintendogs | 24.76 | . Sports Wii Sports | 82.74 | . Strategy Pokemon Stadium | 5.45 | . df_genre.genre.unique() . array([&#39;Sports&#39;, &#39;Platform&#39;, &#39;Racing&#39;, &#39;Role-Playing&#39;, &#39;Puzzle&#39;, &#39;Misc&#39;, &#39;Shooter&#39;, &#39;Simulation&#39;, &#39;Action&#39;, &#39;Fighting&#39;, &#39;Adventure&#39;, &#39;Strategy&#39;], dtype=object) . . df_plat = df[[&#39;name&#39;,&#39;platform&#39;, &#39;global_sales&#39;]] . df_plat.groupby(&#39;platform&#39;).agg({&#39;name&#39;:&#39;first&#39;, &#39;global_sales&#39;:&#39;max&#39;}).sort_values(by=&#39;global_sales&#39;, ascending=False) . name global_sales . platform . Wii Wii Sports | 82.74 | . NES Super Mario Bros. | 40.24 | . GB Pokemon Red/Pokemon Blue | 31.37 | . DS New Super Mario Bros. | 30.01 | . X360 Kinect Adventures! | 21.82 | . PS3 Grand Theft Auto V | 21.40 | . PS2 Grand Theft Auto: San Andreas | 20.81 | . SNES Super Mario World | 20.61 | . GBA Pokemon Ruby/Pokemon Sapphire | 15.85 | . 3DS Pokemon X/Pokemon Y | 14.35 | . PS4 Call of Duty: Black Ops 3 | 14.24 | . N64 Super Mario 64 | 11.89 | . PS Gran Turismo | 10.95 | . XB Halo 2 | 8.49 | . PC The Sims 3 | 8.11 | . 2600 Pac-Man | 7.81 | . PSP Grand Theft Auto: Liberty City Stories | 7.72 | . XOne Call of Duty: Black Ops 3 | 7.30 | . GC Super Smash Bros. Melee | 7.07 | . WiiU Mario Kart 8 | 6.96 | . GEN Sonic the Hedgehog 2 | 6.03 | . DC Sonic Adventure | 2.42 | . PSV Minecraft | 2.25 | . SAT Virtua Fighter 2 | 1.93 | . SCD Sonic CD | 1.50 | . WS Final Fantasy | 0.51 | . NG Samurai Shodown II | 0.25 | . TG16 Doukyuusei | 0.14 | . 3DO Policenauts | 0.06 | . GG Sonic the Hedgehog 2 (8-bit) | 0.04 | . PCFX Blue Breaker: Ken Yorimo Hohoemi o | 0.03 | . . df_unique_game_by_plat = df.groupby(&#39;name&#39;).agg({&#39;platform&#39;:&#39;nunique&#39;}) . df_unique_game_by_plat = df_unique_game_by_plat[df_unique_game_by_plat.platform==1].reset_index() . df_exclusive = df.merge(df_unique_game_by_plat, on=&#39;name&#39;, how=&#39;left&#39;) . df_exclusive.rename(columns={&#39;platform_y&#39;:&#39;is_exclusive&#39;}, inplace=True) df_exclusive.is_exclusive = df_exclusive.is_exclusive.fillna(0) . df_exclusive.groupby([&#39;year&#39;,&#39;is_exclusive&#39;]).sum()[[&#39;global_sales&#39;]].reset_index().head() . year is_exclusive global_sales . 0 1980 | 0.0 | 9.38 | . 1 1980 | 1.0 | 2.00 | . 2 1981 | 0.0 | 5.95 | . 3 1981 | 1.0 | 29.82 | . 4 1982 | 0.0 | 12.78 | . df_exclusive.groupby([&#39;year&#39;,&#39;is_exclusive&#39;]).sum()[[&#39;global_sales&#39;]].reset_index().pivot(&#39;year&#39;,&#39;is_exclusive&#39;,&#39;global_sales&#39;).plot(figsize=(15,10)) plt.title(&#39;Vendas Exclusivos x Não Exclusivos por Ano&#39;) plt.legend([&#39;Não Exclusivo&#39;,&#39;Exclusivo&#39;]) plt.show() . df.head() . rank name platform year genre publisher na_sales eu_sales jp_sales other_sales global_sales . 0 1 | Wii Sports | Wii | 2006 | Sports | Nintendo | 41.49 | 29.02 | 3.77 | 8.46 | 82.74 | . 1 2 | Super Mario Bros. | NES | 1985 | Platform | Nintendo | 29.08 | 3.58 | 6.81 | 0.77 | 40.24 | . 2 3 | Mario Kart Wii | Wii | 2008 | Racing | Nintendo | 15.85 | 12.88 | 3.79 | 3.31 | 35.82 | . 3 4 | Wii Sports Resort | Wii | 2009 | Sports | Nintendo | 15.75 | 11.01 | 3.28 | 2.96 | 33.00 | . 4 5 | Pokemon Red/Pokemon Blue | GB | 1996 | Role-Playing | Nintendo | 11.27 | 8.89 | 10.22 | 1.00 | 31.37 | . df_publisher = df.groupby(&#39;publisher&#39;).agg({&#39;global_sales&#39;:&#39;sum&#39;, &#39;name&#39;:&#39;nunique&#39;}) . df_publisher.sort_values(&#39;global_sales&#39;, ascending=False)[&#39;global_sales&#39;].head().plot.bar() plt.title(&#39;Top 5 Publisher in Global Sales&#39;) plt.show() . df_publisher.sort_values(&#39;name&#39;, ascending=False)[&#39;name&#39;].head().plot.bar() plt.title(&#39;Top 5 Publisher Number of Games&#39;) plt.show() . df[&#39;sales_without_na&#39;] = df[&#39;jp_sales&#39;]+df.eu_sales+df.other_sales . df.groupby(&#39;year&#39;).agg({&#39;sales_without_na&#39;:&#39;sum&#39;, &#39;na_sales&#39;:&#39;sum&#39;, &#39;jp_sales&#39;:&#39;sum&#39;}).plot(figsize=(15,10)) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f7dab4b76d0&gt; . corr = df[[&#39;na_sales&#39;,&#39;sales_without_na&#39;, &#39;eu_sales&#39;,&#39;jp_sales&#39;,&#39;global_sales&#39;]].corr() corr . na_sales sales_without_na eu_sales jp_sales global_sales . na_sales 1.000000 | 0.776859 | 0.768923 | 0.451283 | 0.941269 | . sales_without_na 0.776859 | 1.000000 | 0.932092 | 0.701177 | 0.943837 | . eu_sales 0.768923 | 0.932092 | 1.000000 | 0.436379 | 0.903264 | . jp_sales 0.451283 | 0.701177 | 0.436379 | 1.000000 | 0.612774 | . global_sales 0.941269 | 0.943837 | 0.903264 | 0.612774 | 1.000000 | . sns.heatmap(corr) plt.title(&#39;Correlation&#39;) plt.show() . df.head() . rank name platform year genre publisher na_sales eu_sales jp_sales other_sales global_sales sales_without_na . 0 1 | Wii Sports | Wii | 2006 | Sports | Nintendo | 41.49 | 29.02 | 3.77 | 8.46 | 82.74 | 41.25 | . 1 2 | Super Mario Bros. | NES | 1985 | Platform | Nintendo | 29.08 | 3.58 | 6.81 | 0.77 | 40.24 | 11.16 | . 2 3 | Mario Kart Wii | Wii | 2008 | Racing | Nintendo | 15.85 | 12.88 | 3.79 | 3.31 | 35.82 | 19.98 | . 3 4 | Wii Sports Resort | Wii | 2009 | Sports | Nintendo | 15.75 | 11.01 | 3.28 | 2.96 | 33.00 | 17.25 | . 4 5 | Pokemon Red/Pokemon Blue | GB | 1996 | Role-Playing | Nintendo | 11.27 | 8.89 | 10.22 | 1.00 | 31.37 | 20.11 | . df_genre_by_region = df.groupby(&#39;genre&#39;).sum() df_genre_by_region.head() . rank year na_sales eu_sales jp_sales other_sales global_sales sales_without_na . genre . Action 25955792 | 6527703 | 861.77 | 516.48 | 158.65 | 184.92 | 1722.84 | 860.05 | . Adventure 14704318 | 2558355 | 101.93 | 63.74 | 51.99 | 16.70 | 234.59 | 132.43 | . Fighting 6371780 | 1675871 | 220.74 | 100.00 | 87.15 | 36.19 | 444.05 | 223.34 | . Misc 14445141 | 3384308 | 396.92 | 211.77 | 106.67 | 73.92 | 789.87 | 392.36 | . Platform 6019939 | 1753335 | 445.99 | 200.65 | 130.65 | 51.51 | 829.13 | 382.81 | . df_genre_by_region[[&#39;na_sales&#39;,&#39;eu_sales&#39;, &#39;jp_sales&#39;, &#39;other_sales&#39;]].plot.barh(figsize=(15,10)) plt.show() . Observa&#231;&#227;o . Trechos comentados não rodaram no collab devido ao custo computacional, rodar localmente para comparar distancia entre titulos de jogos. . df.name = df.name.str.lower() . df.name = df.name.str.replace(&#39; &#39;, &#39;_&#39;) . !pip install unidecode . Collecting unidecode Downloading Unidecode-1.3.2-py3-none-any.whl (235 kB) |████████████████████████████████| 235 kB 5.0 MB/s Installing collected packages: unidecode Successfully installed unidecode-1.3.2 . import unidecode . df.name = df.name.apply(unidecode.unidecode) . df_1 = df[[&#39;name&#39;]] df_2 = df[[&#39;name&#39;]] df_1[&#39;key&#39;] = 0 df_2[&#39;key&#39;] = 0 . /usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy This is separate from the ipykernel package so we can avoid doing imports until /usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy after removing the cwd from sys.path. . ## df_matrix = df_2.merge(df_1, how=&#39;outer&#39;, on=&#39;key&#39;, validate=&#39;many_to_many&#39;, suffixes=(&#39;x_&#39;,&#39;y_&#39;)) . # JACCARD # def minhash(input_question, compare_question): # score = 0.0 # shingles = lambda s: set(s[i:i+3] for i in range(len(s)-2)) # jaccard_distance = lambda seta, setb: len(seta &amp; setb)/float(len(seta | setb)) # try: # score = jaccard_distance(shingles(input_question), shingles(compare_question)) # except ZeroDivisionError: # print(&#39;ZeroDivisionError&#39;) # return score # df[&#39;score&#39;] = df.apply(lambda x: minhash(x.x_name, y_name)) . for col in [&#39;na_sales&#39;, &#39;eu_sales&#39;, &#39;jp_sales&#39;, &#39;other_sales&#39;]: print(col) print(df[df[col]==df[col].min()].head(1)) . na_sales rank name ... global_sales sales_without_na 214 215 monster_hunter_freedom_3 ... 4.87 4.87 [1 rows x 12 columns] eu_sales rank name ... global_sales sales_without_na 147 148 final_fantasy_xii ... 5.95 4.07 [1 rows x 12 columns] jp_sales rank name platform ... other_sales global_sales sales_without_na 60 61 just_dance_3 Wii ... 1.07 10.26 4.22 [1 rows x 12 columns] other_sales rank name ... global_sales sales_without_na 137 138 world_of_warcraft ... 6.28 6.21 [1 rows x 12 columns] . Conclus&#245;es . Existem jogos que venderam mais em plataformas mais difundidas | plataformas que venderam aparelhos junto com o jogo atrapalham na contagem | Jogos de estratégia não vendem bem fora do Japão | O mercado Norte Americano é sempre o que mais compra em todas as categorias. Em especial, os de ação. | .",
            "url": "https://vinicius-l-r-matos.github.io/-Repositorio-DS/2022/01/30/_11_20_EDA_Venda_de_jogos.html",
            "relUrl": "/2022/01/30/_11_20_EDA_Venda_de_jogos.html",
            "date": " • Jan 30, 2022"
        }
        
    
  
    
        ,"post4": {
            "title": "EDA Dados SPOTIFY Músicas de 1921 à 2020",
            "content": ". Iremos realizar uma exploração sobre os dados vistos anteriormente do spotify, para visuaalização de gráficos iremos utilizar o SEABORN e o MATPLOTLIB . Abaixo Documentações: . Matplotlib | Seaborn | . Descri&#231;&#227;o Dados . Lembrando sempre de entender o máximo possivel sobre nossos dados temos abaixo a documentação sobre os mesmos. . ACOUSTICNESS Uma medida de confiança de 0,0 a 1,0 para saber se a faixa é acústica. 1.0 representa alta confiança de que a faixa é acústica. . DANCEABILITY . Descreve o quão adequada uma faixa é para dançar com base em uma combinação de elementos musicais, incluindo tempo, estabilidade do ritmo, força da batida e regularidade geral. Um valor de 0,0 é menos dançável e 1,0 é mais dançante. . ENERGY . É uma medida de 0,0 a 1,0 e representa uma medida perceptual de intensidade e atividade. Normalmente, as faixas energéticas parecem rápidas, altas e barulhentas. Por exemplo, death metal tem alta energia, enquanto um prelúdio de Bach tem pontuação baixa na escala. As características perceptivas que contribuem para este atributo incluem faixa dinâmica, intensidade percebida, timbre, taxa de início e entropia geral. . INSTRUMENTALNESS . Prevê se uma faixa não contém vocais. Os sons “Ooh” e “aah” são tratados como instrumentais neste contexto. Faixas de rap ou palavra falada são claramente “vocais”. Quanto mais próximo o valor da instrumentalidade estiver de 1,0, maior será a probabilidade de a faixa não conter conteúdo vocal. Valores acima de 0,5 destinam-se a representar faixas instrumentais, mas a confiança é maior à medida que o valor se aproxima de 1,0. . LIVENESS . Detecta a presença de um público na gravação. Valores de vivacidade mais altos representam um aumento na probabilidade de a trilha ter sido executada ao vivo. Um valor acima de 0,8 fornece uma forte probabilidade de que a faixa esteja ao vivo. . LOUDNESS . O volume geral de uma faixa em decibéis (dB). Os valores de intensidade são calculados em toda a faixa e são úteis para comparar a intensidade relativa das trilhas. Os valores típicos variam entre -60 e 0 db. . SPEECHINESS . A fala detecta a presença de palavras faladas em uma faixa. Quanto mais exclusivamente falada for a gravação (por exemplo, talk show, audiolivro, poesia), mais próximo de 1,0 será o valor do atributo. Valores acima de 0,66 descrevem faixas que provavelmente são compostas inteiramente de palavras faladas. Valores entre 0,33 e 0,66 descrevem faixas que podem conter música e fala, em seções ou em camadas, incluindo casos como rap. . VALENCE . Uma medida de 0,0 a 1,0 que descreve a positividade musical transmitida por uma faixa. Faixas com alta valência soam mais positivas (por exemplo, feliz, alegre, eufórico), enquanto faixas com baixa valência soam mais negativas (por exemplo, triste, deprimido, com raiva). . TEMPO . O tempo estimado geral de uma faixa em batidas por minuto (BPM). Na terminologia musical, o tempo é a velocidade ou ritmo de uma determinada peça e deriva diretamente da duração média do tempo. . KEY . A chave geral estimada da pista. Os inteiros mapeiam para arremessos usando a notação de classe de pitch padrão. Por exemplo. 0 = C, 1 = C♯ / D ♭, 2 = D e assim por diante. Se nenhuma chave for detectada, o valor é -1. . MODE . Modo indica a modalidade (major or minor) de uma faixa, o tipo de escala da qual seu conteúdo melódico é derivado. O major é representado por 1 e o minor é 0. . EXPLICITY . 0 = Não contém conteúdo explicito. 1 = Contém conteúdo explicito. . Link : SPOTIFY KEAGGLE . Posiveis Perguntas . Conseguimos prever a popularidade de uma música pelas caracteristicas ? | Quais músicas possuem maior popularidade ? | Como as variáveis se relacionam com a populariade ? | Há mudança de comportamento/importancia ao longo do tempo ? | Músicas explicitas podem ser mais populares ? | Dançabilidade e Energia são parecidos ? | O sentimento (valencia baixa), e energia, tem o o mode 0 ? | Musicas explicitas tem muita audiencia? | . !pip install -U seaborn . Requirement already satisfied: seaborn in /usr/local/lib/python3.7/dist-packages (0.11.2) Requirement already satisfied: matplotlib&gt;=2.2 in /usr/local/lib/python3.7/dist-packages (from seaborn) (3.2.2) Requirement already satisfied: numpy&gt;=1.15 in /usr/local/lib/python3.7/dist-packages (from seaborn) (1.19.5) Requirement already satisfied: scipy&gt;=1.0 in /usr/local/lib/python3.7/dist-packages (from seaborn) (1.4.1) Requirement already satisfied: pandas&gt;=0.23 in /usr/local/lib/python3.7/dist-packages (from seaborn) (1.1.5) Requirement already satisfied: python-dateutil&gt;=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib&gt;=2.2-&gt;seaborn) (2.8.2) Requirement already satisfied: kiwisolver&gt;=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib&gt;=2.2-&gt;seaborn) (1.3.2) Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,&gt;=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib&gt;=2.2-&gt;seaborn) (3.0.6) Requirement already satisfied: cycler&gt;=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib&gt;=2.2-&gt;seaborn) (0.11.0) Requirement already satisfied: pytz&gt;=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas&gt;=0.23-&gt;seaborn) (2018.9) Requirement already satisfied: six&gt;=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil&gt;=2.1-&gt;matplotlib&gt;=2.2-&gt;seaborn) (1.15.0) . import seaborn as sns import matplotlib.pyplot as plt import pandas as pd import numpy as np . %matplotlib inline . from scipy import stats . O arquivo a ser trabalhado aqui é fruto de uma limoeza de dados duplicados, deu DataSet disponível no Kaggle. . Para seu download direto, pode usar este link . Ou retornar para a postagem onde realizei tal transformação . df = pd.read_csv(&quot;data.csv&quot;) print(df.shape) df.columns . (169909, 19) . Index([&#39;acousticness&#39;, &#39;artists&#39;, &#39;danceability&#39;, &#39;duration_ms&#39;, &#39;energy&#39;, &#39;explicit&#39;, &#39;id&#39;, &#39;instrumentalness&#39;, &#39;key&#39;, &#39;liveness&#39;, &#39;loudness&#39;, &#39;mode&#39;, &#39;name&#39;, &#39;popularity&#39;, &#39;release_date&#39;, &#39;speechiness&#39;, &#39;tempo&#39;, &#39;valence&#39;, &#39;year&#39;], dtype=&#39;object&#39;) . df.sort_values(by=&#39;popularity&#39;, ascending=False, inplace=True) df.head() . acousticness artists danceability duration_ms energy explicit id instrumentalness key liveness loudness mode name popularity release_date speechiness tempo valence year . 87942 0.00146 | [&#39;The Weeknd&#39;] | 0.514 | 200040 | 0.730 | 0 | 0VjIjW4GlUZAMYd2vXMi3b | 0.000095 | 1 | 0.0897 | -5.934 | 1 | Blinding Lights | 100 | 2020-03-20 | 0.0598 | 171.005 | 0.334 | 2020 | . 87940 0.24700 | [&#39;DaBaby&#39;, &#39;Roddy Ricch&#39;] | 0.746 | 181733 | 0.690 | 1 | 7ytR5pFWmSjzHJIeQkgog4 | 0.000000 | 11 | 0.1010 | -7.956 | 1 | ROCKSTAR (feat. Roddy Ricch) | 99 | 2020-04-17 | 0.1640 | 89.977 | 0.497 | 2020 | . 87949 0.73100 | [&#39;Powfu&#39;, &#39;beabadoobee&#39;] | 0.726 | 173333 | 0.431 | 0 | 7eJMfftS33KTjuF7lTsMCx | 0.000000 | 8 | 0.6960 | -8.765 | 0 | death bed (coffee for your head) (feat. beabad... | 97 | 2020-02-08 | 0.1350 | 144.026 | 0.348 | 2020 | . 87941 0.23300 | [&#39;THE SCOTTS&#39;, &#39;Travis Scott&#39;, &#39;Kid Cudi&#39;] | 0.716 | 165978 | 0.537 | 1 | 39Yp9wwQiSRIDOvrVg7mbk | 0.000000 | 0 | 0.1570 | -7.648 | 0 | THE SCOTTS | 96 | 2020-04-24 | 0.0514 | 129.979 | 0.280 | 2020 | . 87852 0.06860 | [&#39;Surf Mesa&#39;, &#39;Emilee&#39;] | 0.674 | 176547 | 0.774 | 0 | 62aP9fBQKYKxi7PDXwcUAS | 0.001880 | 11 | 0.3930 | -7.567 | 0 | ily (i love you baby) (feat. Emilee) | 95 | 2019-11-26 | 0.0892 | 112.050 | 0.330 | 2019 | . df[&#39;duration_minute&#39;] = df.duration_ms/1000/60 . df.duration_minute.hist() . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fb688092e50&gt; . df.head() . acousticness artists danceability duration_ms energy explicit id instrumentalness key liveness loudness mode name popularity release_date speechiness tempo valence year duration_minute . 87942 0.00146 | [&#39;The Weeknd&#39;] | 0.514 | 200040 | 0.730 | 0 | 0VjIjW4GlUZAMYd2vXMi3b | 0.000095 | 1 | 0.0897 | -5.934 | 1 | Blinding Lights | 100 | 2020-03-20 | 0.0598 | 171.005 | 0.334 | 2020 | 3.334000 | . 87940 0.24700 | [&#39;DaBaby&#39;, &#39;Roddy Ricch&#39;] | 0.746 | 181733 | 0.690 | 1 | 7ytR5pFWmSjzHJIeQkgog4 | 0.000000 | 11 | 0.1010 | -7.956 | 1 | ROCKSTAR (feat. Roddy Ricch) | 99 | 2020-04-17 | 0.1640 | 89.977 | 0.497 | 2020 | 3.028883 | . 87949 0.73100 | [&#39;Powfu&#39;, &#39;beabadoobee&#39;] | 0.726 | 173333 | 0.431 | 0 | 7eJMfftS33KTjuF7lTsMCx | 0.000000 | 8 | 0.6960 | -8.765 | 0 | death bed (coffee for your head) (feat. beabad... | 97 | 2020-02-08 | 0.1350 | 144.026 | 0.348 | 2020 | 2.888883 | . 87941 0.23300 | [&#39;THE SCOTTS&#39;, &#39;Travis Scott&#39;, &#39;Kid Cudi&#39;] | 0.716 | 165978 | 0.537 | 1 | 39Yp9wwQiSRIDOvrVg7mbk | 0.000000 | 0 | 0.1570 | -7.648 | 0 | THE SCOTTS | 96 | 2020-04-24 | 0.0514 | 129.979 | 0.280 | 2020 | 2.766300 | . 87852 0.06860 | [&#39;Surf Mesa&#39;, &#39;Emilee&#39;] | 0.674 | 176547 | 0.774 | 0 | 62aP9fBQKYKxi7PDXwcUAS | 0.001880 | 11 | 0.3930 | -7.567 | 0 | ily (i love you baby) (feat. Emilee) | 95 | 2019-11-26 | 0.0892 | 112.050 | 0.330 | 2019 | 2.942450 | . df[df.duration_minute&gt;10] . acousticness artists danceability duration_ms energy explicit id instrumentalness key liveness loudness mode name popularity release_date speechiness tempo valence year duration_minute . 32050 0.002110 | [&#39;TOOL&#39;] | 0.417 | 713192 | 0.529 | 0 | 03sEzk1VyrUZSgyhoQR0LZ | 0.755 | 9 | 0.1100 | -9.338 | 0 | Pneuma | 67 | 2019-08-30 | 0.0315 | 114.116 | 0.0561 | 2019 | 11.886533 | . 83476 0.771000 | [&#39;Pink Floyd&#39;] | 0.266 | 811077 | 0.294 | 0 | 6pnwfWyaWjQiHCKTiZLItr | 0.697 | 7 | 0.1070 | -11.938 | 0 | Shine On You Crazy Diamond (Pts. 1-5) | 66 | 1975-09-12 | 0.0291 | 137.941 | 0.0397 | 1975 | 13.517950 | . 86930 0.020200 | [&#39;Sounds Of Nature : Thunderstorm, Rain&#39;] | 0.217 | 1787761 | 0.911 | 0 | 2AI8gjczczIGkUxospO0MF | 0.724 | 1 | 0.3370 | -27.515 | 1 | Music For Sleep Thunderstorm, Rain | 66 | 2009-05-28 | 0.1800 | 91.211 | 0.0265 | 2009 | 29.796017 | . 47970 0.000824 | [&#39;TOOL&#39;] | 0.313 | 620101 | 0.584 | 0 | 39zWYYZStDgWi32sOU9AX4 | 0.520 | 7 | 0.0529 | -10.091 | 1 | Fear Inoculum | 65 | 2019-08-30 | 0.0415 | 87.894 | 0.1590 | 2019 | 10.335017 | . 24055 0.085000 | [&#39;Lil Darkie&#39;] | 0.457 | 632625 | 0.965 | 1 | 68Y3zzmt2XxzheQVGbbJt4 | 0.000 | 1 | 0.6970 | 0.457 | 1 | GENOCIDE | 64 | 2019-05-20 | 0.4860 | 170.122 | 0.6040 | 2019 | 10.543750 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 145171 0.990000 | [&#39;Robert Schumann&#39;, &#39;Claudio Arrau&#39;, &#39;Karl Kru... | 0.230 | 895920 | 0.186 | 0 | 63kcqKEXHY5QFALveeB8B3 | 0.904 | 0 | 0.1730 | -15.238 | 1 | Piano Concerto in A Minor, Op. 54: I. Allegro ... | 0 | 1941 | 0.0364 | 80.820 | 0.0970 | 1941 | 14.932000 | . 145176 0.911000 | [&quot;Vincent d&#39;Indy&quot;, &#39;Pierre Monteux&#39;] | 0.129 | 712040 | 0.346 | 0 | 66CUjWOqNEK00TSUGabqEj | 0.919 | 2 | 0.0782 | -13.715 | 1 | Symphony No. 2 in B-Flat Major, Op. 57: I. Ext... | 0 | 1941 | 0.0376 | 82.205 | 0.0722 | 1941 | 11.867333 | . 145208 0.960000 | [&quot;Vincent d&#39;Indy&quot;, &#39;Pierre Monteux&#39;, &#39;Maxim Sc... | 0.106 | 671733 | 0.184 | 0 | 6NgY9z6ur5Xdniql1bQ0k7 | 0.826 | 6 | 0.1040 | -18.056 | 1 | Symphonie sur un chant montagnard français, Op... | 0 | 1941 | 0.0395 | 76.389 | 0.0390 | 1941 | 11.195550 | . 145178 0.975000 | [&#39;Ludwig van Beethoven&#39;, &#39;Eugene Ormandy&#39;, &#39;Ph... | 0.293 | 950840 | 0.155 | 0 | 6705OmLjCu99G3aWF4b9jI | 0.740 | 0 | 0.0881 | -15.729 | 0 | Piano Concerto No. 3 in C Minor, Op. 37: I. Al... | 0 | 1941 | 0.0391 | 68.225 | 0.1350 | 1941 | 15.847333 | . 145191 0.800000 | [&#39;Franz Schubert&#39;, &#39;Arturo Toscanini&#39;] | 0.305 | 670280 | 0.235 | 0 | 6DL7WxhCUGl95bOePiE9Q0 | 0.859 | 0 | 0.1270 | -14.211 | 1 | Symphony No. 9 in C Major, D. 944 &quot;The Great&quot;:... | 0 | 1941 | 0.0301 | 103.909 | 0.2340 | 1941 | 11.171333 | . 2253 rows × 20 columns . df.drop(columns=[&#39;id&#39;], inplace=True) . df.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; Int64Index: 169909 entries, 87942 to 0 Data columns (total 19 columns): # Column Non-Null Count Dtype -- -- 0 acousticness 169909 non-null float64 1 artists 169909 non-null object 2 danceability 169909 non-null float64 3 duration_ms 169909 non-null int64 4 energy 169909 non-null float64 5 explicit 169909 non-null int64 6 instrumentalness 169909 non-null float64 7 key 169909 non-null int64 8 liveness 169909 non-null float64 9 loudness 169909 non-null float64 10 mode 169909 non-null int64 11 name 169909 non-null object 12 popularity 169909 non-null int64 13 release_date 169909 non-null object 14 speechiness 169909 non-null float64 15 tempo 169909 non-null float64 16 valence 169909 non-null float64 17 year 169909 non-null int64 18 duration_minute 169909 non-null float64 dtypes: float64(10), int64(6), object(3) memory usage: 25.9+ MB . df[&#39;release_date&#39;] = pd.to_datetime(df[&#39;release_date&#39;] ) df[&#39;year&#39;] = df[&#39;release_date&#39;].dt.year . df.isna().sum() . acousticness 0 artists 0 danceability 0 duration_ms 0 energy 0 explicit 0 instrumentalness 0 key 0 liveness 0 loudness 0 mode 0 name 0 popularity 0 release_date 0 speechiness 0 tempo 0 valence 0 year 0 duration_minute 0 dtype: int64 . df.dropna(inplace=True) . Avalia&#231;&#227;o de Distribui&#231;&#245;es e Densidade . df.popularity.hist(bins=10, density=True) ## Usando matplot # plt.hist(df.popularity, bins=10) # plt.title(&#39;Histograma Popularidade&#39;) plt.show() . plt.figure(figsize =(8,8)) df.boxplot(&#39;popularity&#39;,vert=False) # plt.hist(df.popularity, bins=10) # plt.title(&#39;Histograma Popularidade&#39;) plt.show() . plt.figure(figsize =(8,8)) sns.violinplot(df.popularity) plt.title(&#39;Gráfico de Violino&#39;) plt.show() . /usr/local/lib/python3.7/dist-packages/seaborn/_decorators.py:43: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation. FutureWarning . sns.histplot(data=df, x=&quot;popularity&quot;, bins=10, stat=&quot;density&quot;, kde=True) plt.title(&#39;Distribuição Popularidade&#39;) plt.show() . ## Teste QQPLOT ## Ref - https://data.library.virginia.edu/understanding-q-q-plots/ ## Função scipy https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.probplot.html res = stats.probplot(df[&#39;popularity&#39;], plot=plt) plt.show() . continous_var = [&#39;popularity&#39;, &#39;acousticness&#39;, &#39;danceability&#39;, &#39;duration_ms&#39;, &#39;energy&#39;, &#39;instrumentalness&#39;, &#39;liveness&#39;, &#39;loudness&#39;, &#39;speechiness&#39;, &#39;tempo&#39;, &#39;valence&#39;] . n_rows=3 n_cols=4 # Create the subplots fig, axes = plt.subplots(nrows=n_rows, ncols=n_cols) fig.set_size_inches(20, 15) for i, column in enumerate(continous_var): sns.histplot(df[column], ax=axes[i//n_cols,i%n_cols], bins=10, stat=&quot;density&quot;, kde=True) plt.show() . Associa&#231;&#227;o . corr_var = df[continous_var].corr() plt.figure(figsize = (15,10)) sns.heatmap(data = corr_var, linewidths=.5, annot=True, fmt=&quot;.2f&quot;) plt.show() . Respondendo uma das perguntas feitas no inicio, há uma correlação positiva entre valência e dançabilidade. Parece haver uma forte correlação negativa entre energia e acústica. Vamos também verificar os 10 principais artistas em termos de energia média por música e comparar os resultados com seus valores médios de acústica e popularidade. . df[[&#39;artists&#39;,&#39;energy&#39;,&#39;acousticness&#39;,&#39;popularity&#39;]].groupby(&#39;artists&#39;).mean().sort_values(by=&#39;energy&#39;, ascending=False)[:10] . energy acousticness popularity . artists . [&#39;Ocean Sounds ACE&#39;] 1.000000 | 0.629000 | 58.0 | . [&#39;Darkthrone&#39;] 0.999500 | 0.008002 | 41.0 | . [&#39;Nature Sounds Nature Music&#39;] 0.999333 | 0.640000 | 54.0 | . [&#39;Thunderbound Productions&#39;] 0.999000 | 0.954000 | 63.0 | . [&#39;Tranquility Spree&#39;] 0.999000 | 0.112000 | 68.0 | . [&#39;Swell Maps&#39;] 0.999000 | 0.000298 | 21.0 | . [&#39;The Relaxing Sounds of Swedish Nature&#39;] 0.999000 | 0.508000 | 65.0 | . [&#39;Lluvia PQ&#39;] 0.999000 | 0.123000 | 65.0 | . [&#39;Caramella Girls&#39;] 0.998500 | 0.028700 | 58.5 | . [&#39;Lush Rain Creators&#39;] 0.998000 | 0.141000 | 67.0 | . df.acousticness.mean() . 0.4932139761498759 . Avalia&#231;&#227;o no Tempo . year_avg = df[[&#39;danceability&#39;,&#39;energy&#39;,&#39;liveness&#39;,&#39;acousticness&#39;, &#39;valence&#39;,&#39;year&#39;]].groupby(&#39;year&#39;).mean().sort_values(by=&#39;year&#39;).reset_index() year_avg.head() . year danceability energy liveness acousticness valence . 0 1921 | 0.425661 | 0.236784 | 0.215814 | 0.895823 | 0.425495 | . 1 1922 | 0.480000 | 0.237026 | 0.238647 | 0.939236 | 0.534056 | . 2 1923 | 0.568462 | 0.246936 | 0.236656 | 0.976329 | 0.624788 | . 3 1924 | 0.548654 | 0.347033 | 0.237875 | 0.935575 | 0.668574 | . 4 1925 | 0.571890 | 0.264373 | 0.243094 | 0.965422 | 0.616430 | . plt.figure(figsize=(14,8)) plt.title(&quot;Song Trends Over Time&quot;, fontsize=15) lines = [&#39;danceability&#39;,&#39;energy&#39;,&#39;liveness&#39;,&#39;acousticness&#39;,&#39;valence&#39;] for line in lines: ax = sns.lineplot(x=&#39;year&#39;, y=line, data=year_avg) plt.legend(lines) . &lt;matplotlib.legend.Legend at 0x7fb67cb5fcd0&gt; . Aqui vemos que algumas variaveis crescem ao longo do tempo, essas são energia, valence, dançabilidade. E observamos a acustica como maior queda ao longo dos anos. . Avalia&#231;&#227;o Grupo . sns.boxplot(x=df.explicit, y=df.popularity) plt.title(&#39;Distribuição Músicas Explicitas&#39;) plt.show() . import statsmodels.api as sm import statsmodels.stats.multicomp from statsmodels.formula.api import ols from statsmodels.stats.anova import anova_lm . /usr/local/lib/python3.7/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead. import pandas.util.testing as tm . Aplical&#231;ao T-Test para m&#233;dias . Doc - https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.ttest_ind.html . H0: N&#227;o h&#225; diferen&#231;a entre as m&#233;dias . H1: H&#225; diferen&#231;a . df[df.explicit==0][&#39;popularity&#39;].hist(density=True) df[df.explicit==1][&#39;popularity&#39;].hist(density=True) plt.legend([&#39;explicit 0&#39;, &#39;explicit 1&#39;]) plt.title(&#39;Densidade Música Explicita&#39;) plt.show() . stats.ttest_ind(df[df.explicit==0][&#39;popularity&#39;], df[df.explicit==1][&#39;popularity&#39;]) . Ttest_indResult(statistic=-90.32177595771358, pvalue=0.0) . sns.boxplot(x=df[&#39;mode&#39;], y=df.popularity) plt.title(&#39;Distribuição por Mode&#39;) plt.show() . sns.boxplot(x=df[&#39;mode&#39;], y=df.valence) plt.title(&#39;Distribuição por Mode&#39;) plt.show() . df[df[&#39;mode&#39;]==0][&#39;popularity&#39;].hist(density=True) df[df[&#39;mode&#39;]==1][&#39;popularity&#39;].hist(density=True) plt.legend([&#39;Mode 0&#39;, &#39;Mode 1&#39;]) plt.title(&#39;Densidade Música Explicita&#39;) plt.show() . stats.ttest_ind(df[df[&#39;explicit&#39;]==0][&#39;popularity&#39;], df[df[&#39;mode&#39;]==1][&#39;popularity&#39;]) . Ttest_indResult(statistic=-11.760481179882332, pvalue=6.348401021405311e-32) . sns.boxplot(x=df.key, y=df.popularity) plt.title(&#39;Distribuição Músicas Keys&#39;) plt.show() . ## Doc - https://support.minitab.com/pt-br/minitab/18/help-and-how-to/modeling-statistics/anova/supporting-topics/basics/what-is-anova/ lm = ols(&#39;popularity ~ key&#39;, data=df).fit() table = sm.stats.anova_lm(lm) print(table) . df sum_sq mean_sq F PR(&gt;F) key 1.0 9.018931e+03 9018.930705 19.363948 0.000011 Residual 169907.0 7.913569e+07 465.758879 NaN NaN . Grupos de Popularidade . ## Grupos de Popularidade df[&#39;popularity_group&#39;] = pd.cut(df[&#39;popularity&#39;], 4) df.popularity_group.unique() . [(75.0, 100.0], (50.0, 75.0], (25.0, 50.0], (-0.1, 25.0]] Categories (4, interval[float64]): [(-0.1, 25.0] &lt; (25.0, 50.0] &lt; (50.0, 75.0] &lt; (75.0, 100.0]] . . continous_var.remove(&#39;popularity&#39;) . ValueError Traceback (most recent call last) &lt;ipython-input-51-4ac418754fab&gt; in &lt;module&gt;() -&gt; 1 continous_var.remove(&#39;popularity&#39;) ValueError: list.remove(x): x not in list . n_rows=3 n_cols=4 # Create the subplots fig, axes = plt.subplots(nrows=n_rows, ncols=n_cols) fig.set_size_inches(20, 15) for i, column in enumerate(continous_var): sns.boxplot(x=df.popularity_group, y=df[column], ax=axes[i//n_cols, i%n_cols]) . df_group = df.groupby([&#39;year&#39;,&#39;popularity_group&#39;]).mean().reset_index() . plt.figure(figsize=(14,8)) plt.title(&quot;Acousticness by year&quot;, fontsize=15) sns.lineplot(data=df_group, x=&quot;year&quot;, y=&quot;acousticness&quot;, hue=&quot;popularity_group&quot;) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fb67d039050&gt; . plt.figure(figsize=(14,8)) plt.title(&quot;Energy by year&quot;, fontsize=15) sns.lineplot(data=df_group, x=&quot;year&quot;, y=&quot;energy&quot;, hue=&quot;popularity_group&quot;) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fb66a925d50&gt; . plt.figure(figsize=(14,8)) plt.title(&quot;Danceability by year&quot;, fontsize=15) sns.lineplot(data=df_group, x=&quot;year&quot;, y=&quot;danceability&quot;, hue=&quot;popularity_group&quot;) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fb67cf5c290&gt; . sns.pairplot(df[[&#39;popularity&#39;,&#39;energy&#39;,&#39;danceability&#39;,&#39;acousticness&#39;]]) plt.show() . Conclus&#245;es . Músicas dançantes possuem melhor desempenho . Para uma modelagem futura poderiamos utilizar os dados: . Categoricos: Explicit, Key, Podendo uma das transformações ser a média uma vez que foi validada a diferença. Continuos: Energia, dançabilidade, acustica, loudness. . Como os daos se apresentam em diferentes escala pode ser interessante aplicar alguma normalização. . Podemos também utilizar a variavel de tempo e tratar o problema como uma séries temporal, pois é possivel ver alguns comportamentos de tendencia. . Outras Poss&#237;veis Abordagens . Análise pré 90s x pós 90s. . | Análises por genero. . | .",
            "url": "https://vinicius-l-r-matos.github.io/-Repositorio-DS/fastpages/jupyter/machine%20learning/2022/01/30/_11_20_EDA_Dados_SPOTIFY_M%C3%BAsicas_de_1921_%C3%A0_2020.html",
            "relUrl": "/fastpages/jupyter/machine%20learning/2022/01/30/_11_20_EDA_Dados_SPOTIFY_M%C3%BAsicas_de_1921_%C3%A0_2020.html",
            "date": " • Jan 30, 2022"
        }
        
    
  
    
        ,"post5": {
            "title": "EDA Airbnb",
            "content": ". Perguntas que ira validar . Quais são as regiões mais caras e baratas | Quais as caracteristicas dos imoveis com valor de alugue mais alto | . import pandas as pd import seaborn as sns import matplotlib.pyplot as plt import numpy as np from scipy import stats . %matplotlib inline . airbnb_url = &#39;https://raw.githubusercontent.com/ManarOmar/New-York-Airbnb-2019/master/AB_NYC_2019.csv&#39; df = airbnb_ori = pd.read_csv(airbnb_url) . airbnb_ori.head() . id name host_id host_name neighbourhood_group neighbourhood latitude longitude room_type price minimum_nights number_of_reviews last_review reviews_per_month calculated_host_listings_count availability_365 . 0 2539 | Clean &amp; quiet apt home by the park | 2787 | John | Brooklyn | Kensington | 40.64749 | -73.97237 | Private room | 149 | 1 | 9 | 2018-10-19 | 0.21 | 6 | 365 | . 1 2595 | Skylit Midtown Castle | 2845 | Jennifer | Manhattan | Midtown | 40.75362 | -73.98377 | Entire home/apt | 225 | 1 | 45 | 2019-05-21 | 0.38 | 2 | 355 | . 2 3647 | THE VILLAGE OF HARLEM....NEW YORK ! | 4632 | Elisabeth | Manhattan | Harlem | 40.80902 | -73.94190 | Private room | 150 | 3 | 0 | NaN | NaN | 1 | 365 | . 3 3831 | Cozy Entire Floor of Brownstone | 4869 | LisaRoxanne | Brooklyn | Clinton Hill | 40.68514 | -73.95976 | Entire home/apt | 89 | 1 | 270 | 2019-07-05 | 4.64 | 1 | 194 | . 4 5022 | Entire Apt: Spacious Studio/Loft by central park | 7192 | Laura | Manhattan | East Harlem | 40.79851 | -73.94399 | Entire home/apt | 80 | 10 | 9 | 2018-11-19 | 0.10 | 1 | 0 | . df.shape . (48895, 16) . df.sort_values(&#39;price&#39;, ascending=False, inplace=True) df.head() . id name host_id host_name neighbourhood_group neighbourhood latitude longitude room_type price minimum_nights number_of_reviews last_review reviews_per_month calculated_host_listings_count availability_365 . 9151 7003697 | Furnished room in Astoria apartment | 20582832 | Kathrine | Queens | Astoria | 40.76810 | -73.91651 | Private room | 10000 | 100 | 2 | 2016-02-13 | 0.04 | 1 | 0 | . 29238 22436899 | 1-BR Lincoln Center | 72390391 | Jelena | Manhattan | Upper West Side | 40.77213 | -73.98665 | Entire home/apt | 10000 | 30 | 0 | NaN | NaN | 1 | 83 | . 17692 13894339 | Luxury 1 bedroom apt. -stunning Manhattan views | 5143901 | Erin | Brooklyn | Greenpoint | 40.73260 | -73.95739 | Entire home/apt | 10000 | 5 | 5 | 2017-07-27 | 0.16 | 1 | 0 | . 40433 31340283 | 2br - The Heart of NYC: Manhattans Lower East ... | 4382127 | Matt | Manhattan | Lower East Side | 40.71980 | -73.98566 | Entire home/apt | 9999 | 30 | 0 | NaN | NaN | 1 | 365 | . 6530 4737930 | Spanish Harlem Apt | 1235070 | Olson | Manhattan | East Harlem | 40.79264 | -73.93898 | Entire home/apt | 9999 | 5 | 1 | 2015-01-02 | 0.02 | 1 | 0 | . df.sort_values(&#39;number_of_reviews&#39;, ascending=False, inplace=True) print(df.head()) . id ... availability_365 11759 9145202 ... 333 2031 903972 ... 293 2030 903947 ... 342 2015 891117 ... 339 13495 10101135 ... 173 [5 rows x 16 columns] . df.price.mean() . 152.7206871868289 . df.boxplot(&#39;price&#39;,vert=False) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7ff98bbe4150&gt; . df[df.price&gt;152] . name host_id host_name neighbourhood_group neighbourhood latitude longitude room_type price minimum_nights number_of_reviews last_review reviews_per_month calculated_host_listings_count availability_365 . 4870 Private brownstone studio Brooklyn | 12949460 | Asa | Brooklyn | Park Slope | 40.67926 | -73.97711 | Entire home/apt | 160 | 1 | 488 | 2019-07-01 | 8.14 | 1 | 269 | . 2163 TriBeCa 2500 Sq Ft w/ Priv Elevator | 273174 | Jon | Manhattan | Tribeca | 40.71927 | -74.00453 | Entire home/apt | 575 | 1 | 447 | 2019-07-01 | 5.89 | 3 | 207 | . 1547 NYC 1st Shipping Container Home | 3587751 | Janet-David | Brooklyn | Williamsburg | 40.70995 | -73.95536 | Entire home/apt | 220 | 1 | 404 | 2019-06-25 | 4.90 | 2 | 341 | . 398 ☆Massive DUPLEX☆ 2BR &amp; 2BTH East Village 9+ Gu... | 627217 | Seith | Manhattan | East Village | 40.72939 | -73.98857 | Entire home/apt | 189 | 2 | 403 | 2019-07-07 | 4.10 | 3 | 201 | . 5382 Bright, quiet, cozy 1BR by C Park! | 20116872 | Michael | Manhattan | Upper West Side | 40.77571 | -73.97757 | Entire home/apt | 195 | 1 | 401 | 2019-06-30 | 6.76 | 1 | 178 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 45818 Charming E. Village 2 Bedroom: Steps to the Park! | 263427276 | Helene | Manhattan | East Village | 40.72564 | -73.98030 | Entire home/apt | 300 | 4 | 0 | NaN | NaN | 1 | 297 | . 46743 Amazing 3 BEDROOMS Brooklyn | 217807790 | Jessica | Brooklyn | Williamsburg | 40.71647 | -73.94657 | Entire home/apt | 500 | 3 | 0 | NaN | NaN | 1 | 354 | . 39356 SHOW STOPPER/BEST APARTMENT IN HARLEM | 203982404 | Maxime C/Armande C | Manhattan | East Harlem | 40.80701 | -73.94041 | Entire home/apt | 500 | 2 | 0 | NaN | NaN | 6 | 173 | . 30872 The Madison - A One Bedroom Apartment | 179634496 | Cristian | Manhattan | East Harlem | 40.79983 | -73.94481 | Entire home/apt | 300 | 2 | 0 | NaN | NaN | 1 | 0 | . 47816 True 1-Bedroom in Waterfront Luxury Building | 9618786 | Natalia | Brooklyn | Williamsburg | 40.72003 | -73.96242 | Entire home/apt | 350 | 4 | 0 | NaN | NaN | 1 | 9 | . 14879 rows × 15 columns . . duplicados = df[df.duplicated()] print(duplicados) . Empty DataFrame Columns: [id, name, host_id, host_name, neighbourhood_group, neighbourhood, latitude, longitude, room_type, price, minimum_nights, number_of_reviews, last_review, reviews_per_month, calculated_host_listings_count, availability_365] Index: [] . df.drop(columns=[&#39;id&#39;], inplace=True) . . . plt.figure(figsize =(8,8)) sns.violinplot(df.price) plt.title(&#39;Gráfico de Violino&#39;) plt.show() . /usr/local/lib/python3.7/dist-packages/seaborn/_decorators.py:43: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation. FutureWarning . . sns.histplot(data=df, x=&quot;number_of_reviews&quot;, bins=10, stat=&quot;percent&quot;, kde=True) plt.title(&#39;percentual dos comentários&#39;) plt.show() sns.histplot(data=df, x=&quot;price&quot;, bins=10, stat=&quot;percent&quot;, kde=True) plt.title(&#39;percentual dos preços&#39;) plt.show() . . .",
            "url": "https://vinicius-l-r-matos.github.io/-Repositorio-DS/fastpages/jupyter/machine%20learning/2022/01/30/_11_20_EDA_Airbnb.html",
            "relUrl": "/fastpages/jupyter/machine%20learning/2022/01/30/_11_20_EDA_Airbnb.html",
            "date": " • Jan 30, 2022"
        }
        
    
  
    
        ,"post6": {
            "title": "Como abrir arquivos dentro do Phython",
            "content": ". Para uniciar qualquer estudo sobre o uso de Phyton, é importante conhecer e ser capaz de abrir e explorar diferentes formatos e fontes de dados. . Portanto, para fins de conheciemnto, demonstrar aqui o principal método para se abrir arquivos dentro do colab. Os dados estão em 5 formatos diferentes. . Arquivo = CSV | Imagem = jpg | texto = txt | Repositórios = Dados públicos | Google Sheet = xlsx | . Para confirmação, deixo aqui o link para download dos mesmos: https://drive.google.com/file/d/19slr3OA30rPYIymTzQxql8nNgfPHJkQa/view?usp=sharing . Pr&#233; requisitos: . O resultado fica bem fiel. Para usar, necessita de: . Notebook do Colab aberto | Noções de Python | Conexão com a interet | Arquivos a serem abertos | . Segue o passo a passo de como realizar: . 1 - Montando o Drive: . O primeiro passo é o de montar um arquivo de Drive dentro do colab. Assim é criado o epaço em núvem temposrário que vai abrigar o seu arquivo. . Para isso, usei o comando: . from google.colab import drive drive.mount(&#39;/content/drive&#39;) . Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(&#34;/content/drive&#34;, force_remount=True). . 2 - Importando as bibliotecas: . Duas bibliotecas serão necessárias aqui. Numpy e pandas. Faça agora sua importação: . import numpy as np import pandas as pd . 3 - Carregando as aplica&#231;&#245;es: . Temos diferentes formas de acessar um arquvio para isso precisamos especificar o caminho até o mesmo, para isto temos algumas formas: . documents/folder/arquivo # caminho absoluto #caminhos relativos ./ # pasta atual ../ # pasta anterior . Desta forma, para o Caminho Absoluto Atual, usamos o &quot;!pwd&quot;. . !pwd . /content . Para a Lista de arquivos no diretório atual, usamos o &quot;!ls&quot;. . !ls . csv_file.csv drive image_file.jpg sample_data text_file.txt xlsx_file.xlsx . Para acessar a pasta e operar uploads, se encontra na aba esquerda do Colab, um ícone de Pasta. dentro deste um menu contendo a opção de Upload. Importnte aqui é inserir dentro da pasta padrão &quot;sample_data&quot;. . Os Arquivos usados foram anexados via link no início do artigo, na seção Pré requisitos. . Definimos uma variável para ser acessada com mais facilidade, chamada de &quot;caminho_base&quot;, contendo o diretório dos aquivos na raiz da onde ficam os upload de dentro do Colab. . caminho_base = &#39;./{name_file}&#39; . Para realizar leturas de arquivos do tipo csv, o Pandas necessita declarar o nome do arquivo como variável, seguido da execussão do comando &quot;read_csv(arquivo a ser lido.format(name_file=name_file), sep=&#39;,&#39;, decimal=&#39;.&#39; )&quot;. . Isso deverá trazer o arquivo lido em formato de tabela. . # Doc read_csv - https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html name_file = &#39;csv_file.csv&#39; pd.read_csv(caminho_base.format(name_file=name_file), sep=&#39;,&#39;, decimal=&#39;.&#39; ) . Unnamed: 0 A B C S . 0 0 | -0.498424 | -0.237788 | 0.361243 | -1.593696 | . 1 1 | 1.508797 | -0.811154 | -0.087326 | 0.266954 | . 2 2 | -0.048835 | -0.716396 | 1.390434 | 1.033697 | . 3 3 | 1.612167 | 0.763239 | 0.795475 | 1.740662 | . 4 4 | 0.544064 | 0.478649 | -0.817163 | -0.229374 | . ... ... | ... | ... | ... | ... | . 95 95 | -2.259991 | -1.593170 | -0.026653 | 0.307549 | . 96 96 | 1.100840 | 1.460318 | -1.397809 | -1.460254 | . 97 97 | -0.658386 | -0.203899 | 1.398444 | -0.532728 | . 98 98 | 0.415116 | -0.214596 | -0.388987 | 0.344941 | . 99 99 | -1.112706 | 0.492112 | -0.716792 | 1.032087 | . 100 rows × 5 columns . Para Leitura XLSX, o Pandas necessita declarar o nome do arquivo como variável, seguido da execussão do comando &quot;pd.read_excel(arquivo a ser lido.format(name_file=name_file))&quot;. . Isso deverá trazer o arquivo lido em formato de tabela. . # Doc read_excel https://pandas.pydata.org/docs/reference/api/pandas.read_excel.html name_file = &#39;xlsx_file.xlsx&#39; pd.read_excel(caminho_base.format(name_file=name_file)) . A B C S . 0 -4.984244e-01 | -2.377875e-01 | 3.612434e-01 | -1.593696e+14 | . 1 1.508797e+16 | -8.111543e-01 | -8.732583e-02 | 2.669536e-01 | . 2 -4.883496e-02 | -7.163962e-01 | 1.390434e+14 | 1.033697e+16 | . 3 1.612167e+16 | 7.632387e-01 | 7.954749e-01 | 1.740662e+16 | . 4 5.440637e-01 | 4.786493e-01 | -8.171634e-01 | -2.293735e-01 | . ... ... | ... | ... | ... | . 95 -2.259991e+15 | -1.593170e+16 | -2.665310e-02 | 3.075490e-01 | . 96 1.100840e+16 | 1.460318e+16 | -1.397809e+16 | -1.460254e+16 | . 97 -6.583859e-01 | -2.038990e-01 | 1.398444e+16 | -5.327276e-01 | . 98 4.151162e-01 | -2.145963e-01 | -3.889867e-01 | 3.449411e-01 | . 99 -1.112706e+16 | 4.921122e-01 | -7.167920e-01 | 1.032087e+16 | . 100 rows × 4 columns . Para Leitura txt, o Pandas necessita declarar o nome do arquivo como variável, seguido ou da execussão do comando &quot;f = open(arquivo a ser lido.format(name_file=name_file), &quot;r&quot;) print(f.read()) f.close()&quot;. . Ou para um método com &quot;with&quot;, podemos usar o comando &quot;print(&#39; n 033[1m Titulo a inserir n 033[0m&#39;) with open(arquivo a ser lido.format(name_file=name_file), &#39;r&#39;) as f: lines = f.read() print(lines)&quot; . Isso deverá trazer o arquivo lido em formato de texto. . name_file=&#39;text_file.txt&#39; ##Leitura com with ## Leitura sem with f = open(caminho_base.format(name_file=name_file), &quot;r&quot;) print(f.read()) f.close() ## Leitura com with print(&#39; n 033[1m Leitura com With n 033[0m&#39;) with open(caminho_base.format(name_file=name_file), &#39;r&#39;) as f: lines = f.read() print(lines) . Lorem ipsum dolor sit amet. Et adipisci explicabo ut dolores corrupti est voluptas veritatis. Et veniam illum sed quia voluptatem eum quia consequatur sed beatae quidem ea odit quasi id amet saepe et dignissimos maxime. Ad autem eius eum accusamus animi in iusto velit aut voluptatum dolor sit eligendi modi et perferendis harum quo consectetur velit. Ab adipisci ipsum sed provident ipsam ex beatae voluptatibus. Rem doloribus quia aut aperiam consequuntur hic neque animi et repudiandae laborum eos sunt quibusdam ea dicta vero et consectetur voluptatem. Qui nihil perferendis nam obcaecati maiores ut numquam iste praesentium blanditiis. Vel temporibus officia est eveniet impedit ut iste inventore vel aspernatur nihil est dolor unde. Sit temporibus molestias ut temporibus quia sit obcaecati odit. Ut internos dolorem in repellendus natus cum aspernatur reprehenderit. Aut unde dignissimos quo optio nostrum est nulla corporis sit tempora autem sed optio laudantium ut nobis atque ut quia obcaecati. Vel doloremque quia et culpa molestiae vel corporis fuga eos consequatur quia sit impedit cumque ut sunt provident. Leitura com With Lorem ipsum dolor sit amet. Et adipisci explicabo ut dolores corrupti est voluptas veritatis. Et veniam illum sed quia voluptatem eum quia consequatur sed beatae quidem ea odit quasi id amet saepe et dignissimos maxime. Ad autem eius eum accusamus animi in iusto velit aut voluptatum dolor sit eligendi modi et perferendis harum quo consectetur velit. Ab adipisci ipsum sed provident ipsam ex beatae voluptatibus. Rem doloribus quia aut aperiam consequuntur hic neque animi et repudiandae laborum eos sunt quibusdam ea dicta vero et consectetur voluptatem. Qui nihil perferendis nam obcaecati maiores ut numquam iste praesentium blanditiis. Vel temporibus officia est eveniet impedit ut iste inventore vel aspernatur nihil est dolor unde. Sit temporibus molestias ut temporibus quia sit obcaecati odit. Ut internos dolorem in repellendus natus cum aspernatur reprehenderit. Aut unde dignissimos quo optio nostrum est nulla corporis sit tempora autem sed optio laudantium ut nobis atque ut quia obcaecati. Vel doloremque quia et culpa molestiae vel corporis fuga eos consequatur quia sit impedit cumque ut sunt provident. . Para abrir Imagens, devemos importar uma bibliota chamada &quot;PIL&quot;, o comando &quot;Image&quot;, seguida do nome do arquivo jpg como variável e do comando &quot;im = Image.open(caminho_base.format(name_file=name_file), &#39;r&#39;)&quot;. Além do comando &quot;display(im) im.close()&quot; para tornar visualizavel em qualquer visualizador. . from PIL import Image name_file=&#39;image_file.jpg&#39; # open method used to open different extension image file im = Image.open(caminho_base.format(name_file=name_file), &#39;r&#39;) # This method will show image in any image viewer display(im) im.close() . Para ler Arquivos de Repositórios, basta declarar o url. . url = &#39;https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_confirmed_global.csv&#39; . Seguido do comando &quot;file_web = pd.read_csv(url, error_bad_lines=False)file_web.head()&quot; . file_web = pd.read_csv(url, error_bad_lines=False) file_web.head() . Province/State Country/Region Lat Long 1/22/20 1/23/20 1/24/20 1/25/20 1/26/20 1/27/20 1/28/20 1/29/20 1/30/20 1/31/20 2/1/20 2/2/20 2/3/20 2/4/20 2/5/20 2/6/20 2/7/20 2/8/20 2/9/20 2/10/20 2/11/20 2/12/20 2/13/20 2/14/20 2/15/20 2/16/20 2/17/20 2/18/20 2/19/20 2/20/20 2/21/20 2/22/20 2/23/20 2/24/20 2/25/20 2/26/20 ... 10/12/21 10/13/21 10/14/21 10/15/21 10/16/21 10/17/21 10/18/21 10/19/21 10/20/21 10/21/21 10/22/21 10/23/21 10/24/21 10/25/21 10/26/21 10/27/21 10/28/21 10/29/21 10/30/21 10/31/21 11/1/21 11/2/21 11/3/21 11/4/21 11/5/21 11/6/21 11/7/21 11/8/21 11/9/21 11/10/21 11/11/21 11/12/21 11/13/21 11/14/21 11/15/21 11/16/21 11/17/21 11/18/21 11/19/21 11/20/21 . 0 NaN | Afghanistan | 33.93911 | 67.709953 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 5 | 5 | 5 | ... | 155599 | 155627 | 155682 | 155688 | 155739 | 155764 | 155776 | 155801 | 155859 | 155891 | 155931 | 155940 | 155944 | 156040 | 156071 | 156124 | 156166 | 156196 | 156210 | 156250 | 156284 | 156307 | 156323 | 156363 | 156392 | 156397 | 156397 | 156397 | 156397 | 156414 | 156456 | 156487 | 156510 | 156552 | 156610 | 156649 | 156739 | 156739 | 156812 | 156864 | . 1 NaN | Albania | 41.15330 | 20.168300 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 175664 | 176172 | 176667 | 177108 | 177536 | 177971 | 178188 | 178804 | 179463 | 180029 | 180623 | 181252 | 181696 | 181960 | 182610 | 183282 | 183873 | 184340 | 184887 | 185300 | 185497 | 186222 | 186793 | 187363 | 187994 | 187994 | 189125 | 189355 | 190125 | 190815 | 191440 | 192013 | 192600 | 193075 | 193269 | 193856 | 194472 | 195021 | 195523 | 195988 | . 2 NaN | Algeria | 28.03390 | 1.659600 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 1 | ... | 204790 | 204900 | 205005 | 205106 | 205199 | 205286 | 205364 | 205453 | 205529 | 205599 | 205683 | 205750 | 205822 | 205903 | 205990 | 206069 | 206160 | 206270 | 206358 | 206452 | 206566 | 206649 | 206754 | 206878 | 206995 | 207079 | 207156 | 207254 | 207385 | 207509 | 207624 | 207764 | 207873 | 207970 | 208104 | 208245 | 208380 | 208532 | 208695 | 208839 | . 3 NaN | Andorra | 42.50630 | 1.521800 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 15307 | 15314 | 15326 | 15338 | 15338 | 15338 | 15367 | 15369 | 15382 | 15382 | 15404 | 15404 | 15404 | 15425 | 15425 | 15462 | 15505 | 15516 | 15516 | 15516 | 15516 | 15516 | 15572 | 15618 | 15618 | 15618 | 15618 | 15705 | 15717 | 15744 | 15744 | 15819 | 15819 | 15819 | 15907 | 15929 | 15972 | 16035 | 16086 | 16086 | . 4 NaN | Angola | -11.20270 | 17.873900 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 61794 | 62143 | 62385 | 62606 | 62789 | 62842 | 63012 | 63197 | 63340 | 63567 | 63691 | 63775 | 63861 | 63930 | 64033 | 64126 | 64226 | 64301 | 64374 | 64433 | 64458 | 64487 | 64533 | 64583 | 64612 | 64654 | 64674 | 64724 | 64762 | 64815 | 64857 | 64875 | 64899 | 64913 | 64913 | 64940 | 64968 | 64985 | 64997 | 65011 | . 5 rows × 673 columns . Para abrir arquivos no formato de planilhas da Google, necessitamos do link Público contendoo mesmo (Que pode ser criado no tutorial do - gspread). . Agora declaramos a variável como a url do arquivo e usar o método &quot;sheet_url = &#39;https://*... pd.read_csv(nome do arquivo Google Sheet em url)&quot; . ## Criar Link Publico ou acessar com uso do gspread - https://docs.gspread.org/en/v4.0.1/ sheet_url = &#39;https://docs.google.com/spreadsheets/d/e/2PACX-1vTkXlHy-N6nM18BeZ7J-fONtQ1Ky9K8uZ9jNVHVGZbzCQGC0UoQ7z0nFwxUNQq8jmrDjKkGvuhf5fja/pub?output=csv&#39; pd.read_csv(sheet_url) . A B C S . 0 -0.4984244263085442 | -0.2377875099241897 | 0.3612434266002852 | -159.369.578.501.719 | . 1 15.087.966.605.656.200 | -0.8111543227889415 | -0.08732582822429001 | 0.26695359424512277 | . 2 -0.048834960289222504 | -0.7163961583302045 | 139.043.435.349.605 | 10.336.974.380.283.400 | . 3 16.121.670.473.184.400 | 0.7632387494056144 | 0.7954748670362181 | 17.406.624.041.205.400 | . 4 0.5440636564720198 | 0.47864932161668794 | -0.8171634391145517 | -0.22937354483443884 | . ... ... | ... | ... | ... | . 95 -2.259.991.032.087.180 | -15.931.699.308.172.700 | -0.026653102996842426 | 0.30754901441444293 | . 96 11.008.395.534.521.400 | 14.603.182.990.521.400 | -13.978.086.882.428.800 | -14.602.542.057.378.800 | . 97 -0.6583858789309747 | -0.2038990236295489 | 13.984.444.419.121.300 | -0.5327275728837648 | . 98 0.4151162018084416 | -0.21459625164777488 | -0.3889867047227704 | 0.34494111973103775 | . 99 -11.127.057.849.620.400 | 0.49211223164847473 | -0.7167920305004745 | 10.320.870.274.297.800 | . 100 rows × 4 columns . Conclus&#227;o . Desta forma teremos dados pronto para serem trabalhados com os comandos de Python. . Acredito que desta forma será mais simples usar este processo mais vezes, ou quem sabe passar à frente. . O processo pode ser acessado em Notebook Colab no início desta postagem e reproduzido livremente. .",
            "url": "https://vinicius-l-r-matos.github.io/-Repositorio-DS/fastpages/jupyter/machine%20learning/2022/01/30/_11_20_Como_abrir_arquivos_dentro_do_Phython.html",
            "relUrl": "/fastpages/jupyter/machine%20learning/2022/01/30/_11_20_Como_abrir_arquivos_dentro_do_Phython.html",
            "date": " • Jan 30, 2022"
        }
        
    
  
    
        ,"post7": {
            "title": "Colorizando fotos antigas usando DeOldify",
            "content": ". Essa rede neural colore fotos em preto e branco. . Acredito que possa ser um recurso útil. Uma hora ou outra todos tivemos a curiosidade de vero mundo como nossos antepassados o viam. . O processo aqui consiste em identificar pixel por pixel de uma imagem em escala de cinza, correlacionar e substituir com sua cor RGB correspondente. . Esta ferramente se chama DeOldify. Um projeto open souse. Para maiores informações, basta acasar: . DeOldify . Pr&#233; requisitos: . O resultado fica bem fiel. Para usar, necessita de: . Notebook do Colab aberto | Noções de Python | Conexão com a interet | Url da imagem em preto e branco, em banco de livre acesso | . Segue o passo a passo de como realizar. . 1 - Acessando os reposit&#243;rios: . Para usa-la, necessitamos abrir o github dentro do notebook. Aqui usei o repositório da escola que curso de Ciência de Dados (Awari): . !wget https://raw.githubusercontent.com/awarischool/br-data-science/master/image-colorizer/deoldify_wrapper.py . --2021-11-21 14:09:45-- https://raw.githubusercontent.com/awarischool/br-data-science/master/image-colorizer/deoldify_wrapper.py Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ... Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected. HTTP request sent, awaiting response... 200 OK Length: 2623 (2.6K) [text/plain] Saving to: ‘deoldify_wrapper.py’ deoldify_wrapper.py 100%[===================&gt;] 2.56K --.-KB/s in 0s 2021-11-21 14:09:46 (62.3 MB/s) - ‘deoldify_wrapper.py’ saved [2623/2623] . 2 - Importando as bibliotecas: . Agora, necessitamos importar a biblioteca específica: . from deoldify_wrapper import DeOldify . Importing Libraries No module named &#39;deoldify&#39; DeOldify not found, installing.. Cloning DeOldify Repository... Opening DeOldify Folder Importing Libraries . /content/deoldify_wrapper.py:36: UserWarning: WARNING: GPU not available. Activate it on Colab at Edit &gt; Notebook Settings warnings.warn(&#39;WARNING: GPU not available. Activate it on Colab at Edit &gt; Notebook Settings&#39;) . Installing Colab requirements... Importing DeOldify Visualize module and FastAI Downloading Colorizer Model . 3 - Carregando as aplica&#231;&#245;es: . Agora necessito rodar o seguinte processo: . deo = DeOldify() . Initializing Colorizer . /usr/local/lib/python3.7/dist-packages/fastai/data_block.py:442: UserWarning: Your training set is empty. If this is by design, pass `ignore_empty=True` to remove this warning. warn(&#34;Your training set is empty. If this is by design, pass `ignore_empty=True` to remove this warning.&#34;) /usr/local/lib/python3.7/dist-packages/fastai/data_block.py:445: UserWarning: Your validation set is empty. If this is by design, use `split_none()` or pass `ignore_empty=True` when labelling to remove this warning. or pass `ignore_empty=True` when labelling to remove this warning.&#34;&#34;&#34;) /usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary. cpuset_checked)) Downloading: &#34;https://download.pytorch.org/models/resnet101-63fe2227.pth&#34; to /root/.cache/torch/hub/checkpoints/resnet101-63fe2227.pth . Done! . 4 - Definindo os argumentos: . Para a colorização de Imagem de acesso livre, declaramos agora a url: . url=&#39;https://images.pexels.com/photos/3641670/pexels-photo-3641670.jpeg?cs=srgb&amp;dl=pexels-suzy-hazelwood-3641670.jpg&amp;fm=jpg&#39; . 5 - Visualiza&#231;&#227;o dos dados: . Agora, as imagens serão importadas na seguinte ordem pré definida: . Uma cópia de ressolução grande já transformada | Uma pequena original | Uma outra pequena também já transformada | . Conclus&#227;o . A aplicação roda perfeitamente e possibilita ainda exportar a imagem gerada para demais usos. . Acredito que desta forma será mais simples usar este processo mais vezes, ou quem sabe passar à frente. . O processo pode ser acessado em Notebook Colab no início desta postagem e reproduzido livremente. .",
            "url": "https://vinicius-l-r-matos.github.io/-Repositorio-DS/fastpages/jupyter/machine%20learning/2022/01/30/_11_20_Colorizando_fotos_antigas_usando_DeOldify.html",
            "relUrl": "/fastpages/jupyter/machine%20learning/2022/01/30/_11_20_Colorizando_fotos_antigas_usando_DeOldify.html",
            "date": " • Jan 30, 2022"
        }
        
    
  
    
        ,"post8": {
            "title": "Análise exploratória AirBnb via API",
            "content": ". API é um objeto criado por empresas quando tem a intenção de que outros criadores de software desenvolvam produtos associados ao seu serviço. . Para isso disponibilizam os códigos e instruções para serem usados em outros sites da maneira mais prática para seus usuários. . A palavra API traduzida é algo como Interface de Programação de Aplicativos. . Como exercício realizei toda a preparação e limpeza dos dados do dataset a seguir do Airbnb . Dataset . Este conjunto de dados descreve a atividade de listagem e as métricas em NYC para o ano de 2019. . Este arquivo de dados inclui todas as informações necessárias para descobrir mais sobre hosts, disponibilidade geográfica, métricas necessárias para fazer previsões e tirar conclusões. . Pr&#233; requisitos: . O resultado fica bem fiel. Para usar, necessita de: . Notebook do Colab aberto | Noções de Python | Conexão com a interet | Url da API que deseja usar | . 1 - Importando as bibliotecas: . Duas blibliotecas são necessárias aqui. Pandas e Numpy. . import pandas as pd import numpy as np . 2 - Acessando os reposit&#243;rios: . Declarei a url da API, que no caso é um arquivo csv, como uma variável. . airbnb_url = &#39;https://raw.githubusercontent.com/ManarOmar/New-York-Airbnb-2019/master/AB_NYC_2019.csv&#39; # este é o acesso público para a API airbnb_ori = pd.read_csv(airbnb_url) airbnb = airbnb_ori.copy() wrong_spelling = [&#39;manhatann&#39;, &#39;brookln&#39;, &#39;Blookn&#39;, &#39;Quinns&#39;,&#39;Broonx&#39;] * 10 random_index = airbnb.sample(50, random_state = 10).index airbnb.loc[random_index,&#39;neighbourhood_group&#39;] = wrong_spelling airbnb.loc[random_index,&#39;name&#39;] = &#39;nan&#39; . Ao nomear o aquivo para outra variável, posso realizari comando &quot;print()&quot;, paraconhecer melhor o incio e algumas métricas. . print(airbnb) . id ... availability_365 0 2539 ... 365 1 2595 ... 355 2 3647 ... 365 3 3831 ... 194 4 5022 ... 0 ... ... ... ... 48890 36484665 ... 9 48891 36485057 ... 36 48892 36485431 ... 27 48893 36485609 ... 2 48894 36487245 ... 23 [48895 rows x 16 columns] . 3 - Carregando as aplica&#231;&#245;es: . Declaro uma df, que será a base para relaizar a análise. Ela abre o mesmo arquivo que fiz upload notebook. . df = pd.read_csv(&#39;AB_NYC_2019.csv&#39;, index_col=0) . Tudo certo. Hora de manipular as linhas. . 4 - Definindo os argumentos: . Aqui coloquei na ordem os imóveis com maior quantidade de reviews. . df.sort_values(&#39;number_of_reviews&#39;, ascending=False, inplace=True) print(df.head()) . name ... availability_365 id ... 9145202 Room near JFK Queen Bed ... 333 903972 Great Bedroom in Manhattan ... 293 903947 Beautiful Bedroom in Manhattan ... 342 891117 Private Bedroom in Manhattan ... 339 10101135 Room Near JFK Twin Beds ... 173 [5 rows x 15 columns] . Demonstrndo que propriedades não foram afetadas. . print(df.shape) . (48895, 15) . Agora quero conhecer estatísticas simples com a função &quot;describe()&quot; . df.describe() . host_id latitude longitude price minimum_nights number_of_reviews reviews_per_month calculated_host_listings_count availability_365 . count 4.889500e+04 | 48895.000000 | 48895.000000 | 48895.000000 | 48895.000000 | 48895.000000 | 38843.000000 | 48895.000000 | 48895.000000 | . mean 6.762001e+07 | 40.728949 | -73.952170 | 152.720687 | 7.029962 | 23.274466 | 1.373221 | 7.143982 | 112.781327 | . std 7.861097e+07 | 0.054530 | 0.046157 | 240.154170 | 20.510550 | 44.550582 | 1.680442 | 32.952519 | 131.622289 | . min 2.438000e+03 | 40.499790 | -74.244420 | 0.000000 | 1.000000 | 0.000000 | 0.010000 | 1.000000 | 0.000000 | . 25% 7.822033e+06 | 40.690100 | -73.983070 | 69.000000 | 1.000000 | 1.000000 | 0.190000 | 1.000000 | 0.000000 | . 50% 3.079382e+07 | 40.723070 | -73.955680 | 106.000000 | 3.000000 | 5.000000 | 0.720000 | 1.000000 | 45.000000 | . 75% 1.074344e+08 | 40.763115 | -73.936275 | 175.000000 | 5.000000 | 24.000000 | 2.020000 | 2.000000 | 227.000000 | . max 2.743213e+08 | 40.913060 | -73.712990 | 10000.000000 | 1250.000000 | 629.000000 | 58.500000 | 327.000000 | 365.000000 | . E para conhecer o nome das colunas e quantidade de itens em cada, uso a função &quot;info()&quot; . df.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; Int64Index: 48895 entries, 9145202 to 36487245 Data columns (total 15 columns): # Column Non-Null Count Dtype -- -- 0 name 48879 non-null object 1 host_id 48895 non-null int64 2 host_name 48874 non-null object 3 neighbourhood_group 48895 non-null object 4 neighbourhood 48895 non-null object 5 latitude 48895 non-null float64 6 longitude 48895 non-null float64 7 room_type 48895 non-null object 8 price 48895 non-null int64 9 minimum_nights 48895 non-null int64 10 number_of_reviews 48895 non-null int64 11 last_review 38843 non-null object 12 reviews_per_month 38843 non-null float64 13 calculated_host_listings_count 48895 non-null int64 14 availability_365 48895 non-null int64 dtypes: float64(3), int64(6), object(6) memory usage: 6.0+ MB . Quero ver agora apenas os 5 primeiros valores, com todas as colunas. . df.head(5) . name host_id host_name neighbourhood_group neighbourhood latitude longitude room_type price minimum_nights number_of_reviews last_review reviews_per_month calculated_host_listings_count availability_365 . id . 9145202 Room near JFK Queen Bed | 47621202 | Dona | Queens | Jamaica | 40.66730 | -73.76831 | Private room | 47 | 1 | 629 | 2019-07-05 | 14.58 | 2 | 333 | . 903972 Great Bedroom in Manhattan | 4734398 | Jj | Manhattan | Harlem | 40.82085 | -73.94025 | Private room | 49 | 1 | 607 | 2019-06-21 | 7.75 | 3 | 293 | . 903947 Beautiful Bedroom in Manhattan | 4734398 | Jj | Manhattan | Harlem | 40.82124 | -73.93838 | Private room | 49 | 1 | 597 | 2019-06-23 | 7.72 | 3 | 342 | . 891117 Private Bedroom in Manhattan | 4734398 | Jj | Manhattan | Harlem | 40.82264 | -73.94041 | Private room | 49 | 1 | 594 | 2019-06-15 | 7.57 | 3 | 339 | . 10101135 Room Near JFK Twin Beds | 47621202 | Dona | Queens | Jamaica | 40.66939 | -73.76975 | Private room | 47 | 1 | 576 | 2019-06-27 | 13.40 | 2 | 173 | . E os últimos. . df.tail(5) . name host_id host_name neighbourhood_group neighbourhood latitude longitude room_type price minimum_nights number_of_reviews last_review reviews_per_month calculated_host_listings_count availability_365 . id . 34337032 1 Bed/ 1 Bath / Columbus Circle/ Balcony | 131647128 | Emily | Manhattan | Upper West Side | 40.77076 | -73.98671 | Entire home/apt | 190 | 30 | 0 | NaN | NaN | 25 | 181 | . 34337151 STUNNING 2BR ON MCCARREN PARK WITH PARKING SPOT! | 62014210 | Sara | Brooklyn | Greenpoint | 40.72155 | -73.94671 | Entire home/apt | 700 | 15 | 0 | NaN | NaN | 1 | 24 | . 2958912 STUNNING! 1BD Jr in Midtown East NY | 15100977 | Nataliya | Manhattan | Midtown | 40.75969 | -73.96644 | Entire home/apt | 140 | 300 | 0 | NaN | NaN | 1 | 365 | . 34337805 Ridgewood Love | 7505535 | Magnificent Mohamad | Queens | Ridgewood | 40.70513 | -73.90100 | Private room | 149 | 1 | 0 | NaN | NaN | 1 | 180 | . 36487245 Trendy duplex in the very heart of Hell&#39;s Kitchen | 68119814 | Christophe | Manhattan | Hell&#39;s Kitchen | 40.76404 | -73.98933 | Private room | 90 | 7 | 0 | NaN | NaN | 1 | 23 | . Aquele que possui todos os maiores valores. . df.max() . host_id 274321313 neighbourhood_group Staten Island neighbourhood Woodside latitude 40.9131 longitude -73.713 room_type Shared room price 10000 minimum_nights 1250 number_of_reviews 629 reviews_per_month 58.5 calculated_host_listings_count 327 availability_365 365 dtype: object . Aquele que possui os mínimos. . df.min() . host_id 2438 neighbourhood_group Bronx neighbourhood Allerton latitude 40.4998 longitude -74.2444 room_type Entire home/apt price 0 minimum_nights 1 number_of_reviews 0 reviews_per_month 0.01 calculated_host_listings_count 1 availability_365 0 dtype: object . Se existem valores duplicados. . duplicados = df[df.duplicated()] print(duplicados) . Empty DataFrame Columns: [name, host_id, host_name, neighbourhood_group, neighbourhood, latitude, longitude, room_type, price, minimum_nights, number_of_reviews, last_review, reviews_per_month, calculated_host_listings_count, availability_365] Index: [] . Nenhum. Ótimo! Mas ainda podem existir nomes duplicados. . print(df[df.duplicated(subset=[&#39;name&#39;])]) . name ... availability_365 id ... 9434513 2000sq $2 million 3 story townhouse ... 363 1330056 Spacious Loft 5 min to Union Square ... 359 482765 Beautiful Brooklyn Brownstone ... 12 18946416 Beautiful Brooklyn Brownstone ... 274 18091991 Cozy Room ... 128 ... ... ... ... 35461209 Freshly furnished private room - GREAT Location! ... 219 2925138 Steps from Central Park ... 0 35461527 Artsy Private BR in Fort Greene Cumberland ... 342 35461700 Artsy Private BR in Fort Greene Cumberland ... 341 35461620 Artsy Private BR in Fort Greene Cumberland ... 305 [989 rows x 15 columns] . Contabilizadas 989 ocorrências. . Conclus&#227;o . A aplicação roda perfeitamente e possibilita ainda exportar a dados gerados para demais usos. . Acredito que desta forma será mais simples usar este processo mais vezes, ou quem sabe passar à frente. . O processo pode ser acessado em Notebook Colab no início desta postagem e reproduzido livremente. . A API tem quantidade relevante de nomes duplicados. Pode afetar análises que deendam disso. .",
            "url": "https://vinicius-l-r-matos.github.io/-Repositorio-DS/fastpages/jupyter/data%20exploration/2022/01/30/_11_20_An%C3%A1lise_explorat%C3%B3ria_AirBnb_via_API.html",
            "relUrl": "/fastpages/jupyter/data%20exploration/2022/01/30/_11_20_An%C3%A1lise_explorat%C3%B3ria_AirBnb_via_API.html",
            "date": " • Jan 30, 2022"
        }
        
    
  
    
        ,"post9": {
            "title": "Visualizações com Matplot",
            "content": ". Considera&#231;&#245;es iniciais . O objetivo deste notebook é fazer com que o leitor assimile os recursos de uma das bibliotecas mais importantes do Python: o Matplotlib. Para isso, exemplos e conceitos desta biblioteca serão apresentados ao longo da minha jornada. Concomitantemente, serão apresentadas algumas tarefas práticas ao leitor. . Por que o Matplotlib? . Apesar do Python possuir um ecossitema de feramentas de visualização muito rico para o cientista de dados, o Matplotlib, sem dúvidas, também detém seu lugar de destaque. . &quot;Matplolib tenta facilitar as coisas fáceis e tornar as coisas difíceis possíveis.&quot; Site do Matplotlib Além do Matplotlib, utilizaremos, também, um conjunto de dados e outras bibliotecas de apoio, como o Pandas, por exemplo. Afinal, o Matplotlib é ótimo, mas não brilha sozinho. . Prepare seu ambiente . Se necessário, faça a instalação da biblioteca no seu sistema usando o gerenciador de pacotes da linguagem, no prompt/terminal/cmd digite: . $ pip install matplotlib . Ou caso esteja usando o Anaconda:&gt;$ conda install matplotlib . Conjunto de dados . O conjunto de dados utilizado são os registros de milhares de lutas no Ultimate Fight Championship (UFC). O arquivo ufc.csv possui mais de 145 colunas, o que é demasiadamente grande para as nossas intenções. Deste modo, iremos focar apenas uma parte dos dados. . Descri&#231;&#227;o do dados . De todas as colunas do arquivo ufc.csv, iremos praticar com: . R_fighter: Nome do lutador do canto vermelho. | B_fighter: Nome do lutador do canto azul. | Referee: Nome do árbitro da luta. | date: Data do evento | location: Local do evento. | Winner: Cor do vencedor - Red ou Blue. | title_bout: Se é uma disputa pelo título. | weight_class: Classe de peso da luta. | . Procedimentos . Vamos abordar alguns padrões básicos de uso e práticas recomendadas para ajudar você a utilizar o Matplotlib. Por ter um código bastante extenso, o Matplotlib pode ser um desafio ultrajante (e é) para os iniciantes, mas não se intimide. A intenção deste guia é, justamente, focar no essencial para plotagem dos primeiros gráficos sem necessitar de muita informação. . Importando as bibliotecas . %matplotlib inline: é uma Magic Word do Jupyter Notebook que informa à plataforma que informa ao Jupyter para que os gráficos sejam plotados diratemente no notebook. . | numpy: Uma biblioteca de apoio para gerar alguns dados para plotagem. pandas: A biblioteca pandas fornece estruturas e ferramentas de análise de dados. Iremos utilizá-la para carregar e manipular o conjunto de dados em uma estrutura chamada dataframe. Por convenção, recomenda-se o pandas seja utilizado com o nome pd. . | matplotlib.pyplot: A estrela de hoje. Por convenção, recomenda-se que matplotlib.pyplot seja acessado via pelo nome plt. . | . %matplotlib inline import numpy as np import pandas as pd import matplotlib.pyplot as plt . TAREFA 01 . Importe o arquivo ufc.csv em um dataframe. Visualize as primeiras 5 linhas do dataframe . Importe o arquivo ufc.csv em um dataframe. | . df = pd.read_csv(&#39;https://raw.githubusercontent.com/awarischool/visualizacao_exploratoria_exercicios/69c68b830d6f4b9f6f5d938d131fab47af39adf0/01-exercicios-matplotlib/ufc.csv&#39;) . Visualize as primeiras 5 linhas do dataframe | . df.head() . R_fighter B_fighter Referee date location Winner title_bout weight_class no_of_rounds B_current_lose_streak B_current_win_streak B_draw B_avg_BODY_att B_avg_BODY_landed B_avg_CLINCH_att B_avg_CLINCH_landed B_avg_DISTANCE_att B_avg_DISTANCE_landed B_avg_GROUND_att B_avg_GROUND_landed B_avg_HEAD_att B_avg_HEAD_landed B_avg_KD B_avg_LEG_att B_avg_LEG_landed B_avg_PASS B_avg_REV B_avg_SIG_STR_att B_avg_SIG_STR_landed B_avg_SIG_STR_pct B_avg_SUB_ATT B_avg_TD_att B_avg_TD_landed B_avg_TD_pct B_avg_TOTAL_STR_att B_avg_TOTAL_STR_landed B_longest_win_streak B_losses B_avg_opp_BODY_att B_avg_opp_BODY_landed ... R_avg_opp_BODY_att R_avg_opp_BODY_landed R_avg_opp_CLINCH_att R_avg_opp_CLINCH_landed R_avg_opp_DISTANCE_att R_avg_opp_DISTANCE_landed R_avg_opp_GROUND_att R_avg_opp_GROUND_landed R_avg_opp_HEAD_att R_avg_opp_HEAD_landed R_avg_opp_KD R_avg_opp_LEG_att R_avg_opp_LEG_landed R_avg_opp_PASS R_avg_opp_REV R_avg_opp_SIG_STR_att R_avg_opp_SIG_STR_landed R_avg_opp_SIG_STR_pct R_avg_opp_SUB_ATT R_avg_opp_TD_att R_avg_opp_TD_landed R_avg_opp_TD_pct R_avg_opp_TOTAL_STR_att R_avg_opp_TOTAL_STR_landed R_total_rounds_fought R_total_time_fought(seconds) R_total_title_bouts R_win_by_Decision_Majority R_win_by_Decision_Split R_win_by_Decision_Unanimous R_win_by_KO/TKO R_win_by_Submission R_win_by_TKO_Doctor_Stoppage R_wins R_Stance R_Height_cms R_Reach_cms R_Weight_lbs B_age R_age . 0 Henry Cejudo | Marlon Moraes | Marc Goddard | 2019-06-08 | Chicago, Illinois, USA | Red | True | Bantamweight | 5 | 0.0 | 4.0 | 0.0 | 9.200000 | 6.000000 | 0.200000 | 0.000000 | 62.600000 | 20.600000 | 2.600000 | 2.000000 | 48.600000 | 11.200000 | 0.800000 | 7.6 | 5.400000 | 0.400000 | 0.000000 | 65.40 | 22.600000 | 0.466000 | 0.400000 | 0.80000 | 0.200000 | 0.100000 | 66.400000 | 23.600000 | 4.0 | 1.0 | 6.400000 | 4.000000 | ... | 13.300000 | 8.800000 | 7.500000 | 5.100000 | 90.500000 | 26.800000 | 0.800000 | 0.300000 | 76.100000 | 17.300000 | 0.100000 | 9.400000 | 6.100000 | 0.000000 | 0.000000 | 98.800000 | 32.200000 | 0.336000 | 0.000000 | 0.900000 | 0.100000 | 0.050000 | 110.500000 | 43.300000 | 27.0 | 742.60 | 3.0 | 0.0 | 2.0 | 4.0 | 2.0 | 0.0 | 0.0 | 8.0 | Orthodox | 162.56 | 162.56 | 135.0 | 31.0 | 32.0 | . 1 Valentina Shevchenko | Jessica Eye | Robert Madrigal | 2019-06-08 | Chicago, Illinois, USA | Red | True | Women&#39;s Flyweight | 5 | 0.0 | 3.0 | 0.0 | 14.600000 | 9.100000 | 11.800000 | 7.300000 | 124.700000 | 42.100000 | 2.400000 | 1.900000 | 112.000000 | 32.000000 | 0.000000 | 12.3 | 10.200000 | 0.800000 | 0.000000 | 138.90 | 51.300000 | 0.399000 | 0.700000 | 1.00000 | 0.500000 | 0.225000 | 158.700000 | 69.600000 | 3.0 | 6.0 | 13.000000 | 9.300000 | ... | 24.571429 | 14.142857 | 10.571429 | 7.857143 | 98.571429 | 32.571429 | 6.428571 | 4.285714 | 61.857143 | 12.428571 | 0.000000 | 29.142857 | 18.142857 | 1.142857 | 0.000000 | 115.571429 | 44.714286 | 0.437143 | 0.285714 | 3.285714 | 0.857143 | 0.147143 | 158.142857 | 82.285714 | 25.0 | 1062.00 | 2.0 | 0.0 | 1.0 | 2.0 | 0.0 | 2.0 | 0.0 | 5.0 | Southpaw | 165.10 | 167.64 | 125.0 | 32.0 | 31.0 | . 2 Tony Ferguson | Donald Cerrone | Dan Miragliotta | 2019-06-08 | Chicago, Illinois, USA | Red | False | Lightweight | 3 | 0.0 | 3.0 | 0.0 | 15.354839 | 11.322581 | 6.741935 | 4.387097 | 84.741935 | 38.580645 | 5.516129 | 3.806452 | 67.645161 | 23.258065 | 0.645161 | 14.0 | 12.193548 | 0.935484 | 0.096774 | 97.00 | 46.774194 | 0.496129 | 0.354839 | 2.16129 | 0.677419 | 0.295484 | 103.709677 | 52.548387 | 8.0 | 8.0 | 17.903226 | 11.870968 | ... | 14.466667 | 8.133333 | 2.800000 | 0.733333 | 91.066667 | 32.200000 | 4.866667 | 2.800000 | 78.266667 | 23.200000 | 0.266667 | 6.000000 | 4.400000 | 0.333333 | 0.133333 | 98.733333 | 35.733333 | 0.340000 | 0.066667 | 2.866667 | 0.666667 | 0.131333 | 102.133333 | 38.600000 | 33.0 | 604.40 | 2.0 | 0.0 | 1.0 | 3.0 | 3.0 | 6.0 | 1.0 | 14.0 | Orthodox | 180.34 | 193.04 | 155.0 | 36.0 | 35.0 | . 3 Jimmie Rivera | Petr Yan | Kevin MacDonald | 2019-06-08 | Chicago, Illinois, USA | Blue | False | Bantamweight | 3 | 0.0 | 4.0 | 0.0 | 17.000000 | 14.000000 | 13.750000 | 11.000000 | 109.500000 | 48.750000 | 13.000000 | 10.500000 | 116.250000 | 53.750000 | 0.500000 | 3.0 | 2.500000 | 0.500000 | 0.250000 | 136.25 | 70.250000 | 0.550000 | 0.250000 | 2.50000 | 1.250000 | 0.287500 | 154.750000 | 86.750000 | 4.0 | 0.0 | 12.250000 | 6.000000 | ... | 20.250000 | 13.375000 | 6.875000 | 5.625000 | 103.125000 | 38.500000 | 0.875000 | 0.750000 | 77.375000 | 20.375000 | 0.125000 | 13.250000 | 11.125000 | 0.000000 | 0.000000 | 110.875000 | 44.875000 | 0.446250 | 0.000000 | 2.375000 | 0.000000 | 0.000000 | 115.125000 | 48.875000 | 20.0 | 690.25 | 0.0 | 0.0 | 1.0 | 4.0 | 1.0 | 0.0 | 0.0 | 6.0 | Orthodox | 162.56 | 172.72 | 135.0 | 26.0 | 29.0 | . 4 Tai Tuivasa | Blagoy Ivanov | Dan Miragliotta | 2019-06-08 | Chicago, Illinois, USA | Blue | False | Heavyweight | 3 | 0.0 | 1.0 | 0.0 | 17.000000 | 14.500000 | 2.500000 | 2.000000 | 201.000000 | 59.500000 | 0.000000 | 0.000000 | 184.500000 | 45.000000 | 0.000000 | 2.0 | 2.000000 | 0.000000 | 0.000000 | 203.50 | 61.500000 | 0.310000 | 0.000000 | 0.00000 | 0.000000 | 0.000000 | 204.000000 | 62.000000 | 1.0 | 1.0 | 42.500000 | 23.500000 | ... | 6.250000 | 4.750000 | 4.500000 | 3.500000 | 42.750000 | 16.250000 | 7.750000 | 2.750000 | 43.250000 | 14.000000 | 0.250000 | 5.500000 | 3.750000 | 0.750000 | 0.000000 | 55.000000 | 22.500000 | 0.397500 | 0.000000 | 1.000000 | 0.000000 | 0.000000 | 60.500000 | 27.750000 | 7.0 | 440.75 | 0.0 | 0.0 | 0.0 | 1.0 | 2.0 | 0.0 | 0.0 | 3.0 | Southpaw | 187.96 | 190.50 | 264.0 | 32.0 | 26.0 | . 5 rows × 145 columns . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; Diferen&#231;as entre os m&#233;todos plt.show() e plt.plot() . Caso você esteja executando matplotlib a partir do script Python, use o plt.show() em qualquer plotagem. Porém, no nosso caso, rodando a partir do notebook, nosso trabalho poderá ser realizado apenas com plt.plot(). Não se esqueça que para isso, ativamos a opção de apresentar os gráficos diretamente no notebook através do comando %matplotlib inline. . A anatomia de um gr&#225;fico . O Matplotlib é baseado no conceito de que todos os elementos que constituem um gráfico estão organizados dentro de uma hierarquia. No topo desta hirarquia está o ambiente do Matplotlib, fornecido pelo módulo que importamos anteriormente - matplotlib.pyplot. A partir dele, o usuário acessa funções que adicionam elementos nos gráficos (linhas, imagens, legendas, etc). . Descendo na hirarquia, o usuário poderá utilizar objetos que possuem funcionalidades. Por exemplo, uma figura é criada a partir de um objeto Figure ou um subgráfico em um gráfico é manipulado por um objeto Axe. Aliás, no fim das contas, para deixar o código mais &quot;acessível&quot;, o usuário acaba sempre trabalhando com os objetos do Matplotlib. . Para esclarecer um pouco mais, observe a imagem abaixo. Nela, estão contidas os elementos que compoem um gráfico: . Não se preocupe em entender cada um dos elementos, o importante é ter a noção de que estes elementos possuem uma hierarquia na organização e que você deve obedecê-la durante a montagem de gráficos mais complexos. . Figure . O Figure é o elemento de mais alto nível do gráfico. Uma figura pode ter vários eixos (Axes), mas um eixo pode pertencer a somente uma figura. Além disso, o objeto Figure aceita parâmetros que formatam nosso gráfico. . TAREFA 02 . Instancie uma figura. Use o plt.plot() para visualizar no notebook. . DICA: O objeto figure está dentro do matplotlib.pyplot. | DICA: Caso você não tenha usado o comando %matplotlib inline, a figura não aparecerá. | . fig = plt.figure() ax = plt.axes() plt.plot() . [] . TAREFA 03 . Instancie uma figura alterando seu tamanho. . Use o plt.plot() para visualizar no notebook. | DICA: Use o parâmetro figsize. | . fig = plt.figure(figsize=(10,10)) ax = plt.axes() plt.plot() . [] . Axe . Pense em um Axe como uma região da figura onde você plotará seus gráficos. Uma Figure pode ter vários Axes, mas um Axe pertence a uma só Figure. Para adicionar Axes à uma Figure, podemos utilizar os métodos fig.add_axes() ou fig.add_subplot(). Para esclarecer, Axes e subplots são a mesma coisa. . O método fig.add_subplots() usa 3 números inteiros como parâmetro. Esses três números definem o número de linhas e colunas e a posição do subplot na figura. Por exemplo, usando fig.add_subplots(ijk) adiciona um eixo na k-enésima posição de uma grade que possui i linhas e j colunas. . O método fig.add_subplot() é a maneira mais fácil de configurar seu layout, enquanto fig.add_axes() lhe dará mais controle sobre a posição dos seus eixos. . TAREFA 04 . Instancie uma figura | Adicione dois subplots lado a lado. | Não se esqueça do plot.plot() | . DICA: Utilize o método fig.add_axes(). . fig = plt.figure() fig.add_subplot(121) fig.add_subplot(122) plt.plot() . [] . Cada subplot contém dois eixos, representados pelos objetos Axis (observe a diferença entre Axes e Axis) que cuidam dos limites de cada subplot. As faixas de valores dos eixos podem ser controlados através dos métodos axes.set_xlim() e axes.set_ylim(). . TAREFA 05 . Altere os limites dos subplotes da figura anterior | Para o subplot da esquerda, configure o eixo x de 0 a 10. | No subplot da direita, defina y entre -1 e 1. | Adicione os títulos &quot;Esquerda&quot; e &quot;Direita&quot; nos subplots. | . fig = plt.figure() fig.add_subplot(121) plt.xlim(0,10) plt.title(&#39;Esquerda&#39;) fig.add_subplot(122) plt.ylim(-1,1) plt.title(&#39;Direita&#39;) plt.plot() . [] . Nossos subplots estão sobreajustados e o eixo y do subplot da direita está ruim de visualizar. . TAREFA 06 . Com base na figura anterior, ajuste o gráfico. | Ajuste o tamanho da figura. | . DICA: Use o método figure.tight_layout(). . fig = plt.figure(figsize=(8,4)) fig.add_subplot(121) plt.xlim(0,10) plt.title(&#39;Esquerda&#39;) fig.add_subplot(122) plt.ylim(-1,1) plt.title(&#39;Direita&#39;) fig.tight_layout() plt.plot() . [] . Bem melhor não é? Percebeu como os objetos (Figure, Axes, etc) são utilizados na montagem do gráfico? E como os atritutos (figsize) e métodos (tight_layout(), set_title(), set_xlim(), etc) destes objetos são utilizados para alterar a estética do grafico? . Os recursos do Matplotlib são vastos, mas o conceito de hirarquia de elementos das figuras e a interação destes elementos farão você chegar à qualquer lugar na hora de montar seu gráfico. . Plotando seus gr&#225;ficos . Vamos voltar ao conjunto de dados do UFC que carregamos mais cedo. Afinal, o Matplotlib é somente uma tela em branco quando você não tem o que desenhar. . A variável fights_by_year armazena a quantidade de lutas por ano do evento. Apesar deste notebook não pretender ensinar Pandas, é altamente recomendável que você não tenha dúvida sobre o que foi feito na célula abaixo. . df[&#39;date&#39;] = pd.to_datetime(df[&#39;date&#39;]) fights_by_year = df.groupby(df[&#39;date&#39;].dt.year).size() . TAREFA 07 . Plote a evolução das quantidades de lutas por ano no UFC. | Use um gráfico de linha. | Configure título e nome dos eixos x e y. | . plt.plot(fights_by_year) plt.title(&#39;Evolução da Quantidade de Lutas do UFC&#39;) plt.ylabel(&#39;Qtde. de Lutas&#39;) plt.xlabel(&#39;Ano&#39;) plt.plot() . [] . Matplotlib e pandas . A biblioteca Pandas possui integração com o Matplotlib, permitindo que sejam criados gráficos diretamente dos dataframes do Pandas. Se você quiser plotar as informações de forma mais rápida, basta você utilizar o método plot() do próprio dataframe. . Separamos as 10 classes de peso mais relevantes - maior quantidade de lutas - do UFC ao longo dos anos. . fights_by_class = df[&#39;weight_class&#39;].value_counts()[:10] . TAREFA 08 . Plote a quantidade de lutas das 10 classes de peso mais relevantes do UFC. | Use um gráfico de barra diretamente do dataframe. | Armazene seu plot em um objeto Axe. | Configure título e nome dos eixos x e y. | . plt.bar(x=fights_by_class.index, height=fights_by_class.values, width=0.6) plt.title(&#39;Quantidade de Lutas por Classe de Peso&#39;) plt.ylabel(&#39;Qtd. de Lutas&#39;) plt.xlabel(&#39;Classe de Peso&#39;) plt.xticks(rotation=60) plt.plot() . [] . Conclus&#227;o . Matplotlib é provavelmente o pacote Python mais utilizado para gráficos 2D. Ele fornece uma maneira muito rápida de visualizar os dados de Python e figuras de qualidade de publicação em vários formatos. Em conjunto com outras ferramentas, como Pandas e Seaborn (este é um complemento ao Matplotlib), os cientistas de dados tem um grande aliado. .",
            "url": "https://vinicius-l-r-matos.github.io/-Repositorio-DS/fastpages/jupyter/data%20exploration/2022/01/30/_01_30_Visualiza%C3%A7%C3%B5es_com_Matplot.html",
            "relUrl": "/fastpages/jupyter/data%20exploration/2022/01/30/_01_30_Visualiza%C3%A7%C3%B5es_com_Matplot.html",
            "date": " • Jan 30, 2022"
        }
        
    
  
    
        ,"post10": {
            "title": "Como manipular dados acessando API em Github",
            "content": ". Exerc&#237;cios de manipula&#231;&#227;o de dados - Parte 1 . ! git clone https://github.com/Mario-RJunior/data-manipulation-exercises . Cloning into &#39;data-manipulation-exercises&#39;... remote: Enumerating objects: 31, done. remote: Total 31 (delta 0), reused 0 (delta 0), pack-reused 31 Unpacking objects: 100% (31/31), done. . cd data-manipulation-exercises/ . /content/data-manipulation-exercises . ls . datasets/ Manipulacao_de_Dados_Ex_02.ipynb README.md Manipulacao_de_Dados_Ex_01.ipynb Manipulacao_de_Dados_Ex_03.ipynb . import pandas as pd . Passo 1. Importando os dados . Carregue os dados salvos no arquivo datasets/users_dataset.csv. . Esse arquivo possui um conjunto de dados de trabalhadores com 5 colunas separadas pelo símbolo &quot;|&quot; (pipe) e 943 linhas. . Dica: não se esqueça do argumento sep quando importar os dados. . df = pd.read_csv(&#39;datasets/users_dataset.csv&#39;, sep=&#39;|&#39;) . Passo 2. Mostre as 15 primeiras linhas do dataset. . df.head(15) . user_id age gender occupation zip_code . 0 1 | 24 | M | technician | 85711 | . 1 2 | 53 | F | other | 94043 | . 2 3 | 23 | M | writer | 32067 | . 3 4 | 24 | M | technician | 43537 | . 4 5 | 33 | F | other | 15213 | . 5 6 | 42 | M | executive | 98101 | . 6 7 | 57 | M | administrator | 91344 | . 7 8 | 36 | M | administrator | 05201 | . 8 9 | 29 | M | student | 01002 | . 9 10 | 53 | M | lawyer | 90703 | . 10 11 | 39 | F | other | 30329 | . 11 12 | 28 | F | other | 06405 | . 12 13 | 47 | M | educator | 29206 | . 13 14 | 45 | M | scientist | 55106 | . 14 15 | 49 | F | educator | 97301 | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; Passo 3. Mostre as 10 &#250;ltimas linhas . users.tail(10) . user_id age gender occupation zip_code . 933 934 | 61 | M | engineer | 22902 | . 934 935 | 42 | M | doctor | 66221 | . 935 936 | 24 | M | other | 32789 | . 936 937 | 48 | M | educator | 98072 | . 937 938 | 38 | F | technician | 55038 | . 938 939 | 26 | F | student | 33319 | . 939 940 | 32 | M | administrator | 02215 | . 940 941 | 20 | M | student | 97229 | . 941 942 | 48 | F | librarian | 78209 | . 942 943 | 22 | M | student | 77841 | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; Passo 4. Qual o n&#250;mero de linhas e colunas do DataFrame? . df.shape . (943, 5) . Passo 5. Mostre o nome de todas as colunas. . users.columns . Index([&#39;user_id&#39;, &#39;age&#39;, &#39;gender&#39;, &#39;occupation&#39;, &#39;zip_code&#39;], dtype=&#39;object&#39;) . Passo 6. Qual o tipo de dado de cada coluna? . df.dtypes . user_id int64 age int64 gender object occupation object zip_code object dtype: object . Passo 7. Mostre os dados da coluna occupation. . df[&#39;occupation&#39;] . 0 technician 1 other 2 writer 3 technician 4 other ... 938 student 939 administrator 940 student 941 librarian 942 student Name: occupation, Length: 943, dtype: object . Passo 8. Quantas ocupa&#231;&#245;es diferentes existem neste dataset? . print(df[&#39;occupation&#39;].unique()) len(df[&#39;occupation&#39;].unique()) . [&#39;technician&#39; &#39;other&#39; &#39;writer&#39; &#39;executive&#39; &#39;administrator&#39; &#39;student&#39; &#39;lawyer&#39; &#39;educator&#39; &#39;scientist&#39; &#39;entertainment&#39; &#39;programmer&#39; &#39;librarian&#39; &#39;homemaker&#39; &#39;artist&#39; &#39;engineer&#39; &#39;marketing&#39; &#39;none&#39; &#39;healthcare&#39; &#39;retired&#39; &#39;salesman&#39; &#39;doctor&#39;] . 21 . Passo 9. Qual a ocupa&#231;&#227;o mais frequente? . df[&#39;occupation&#39;].value_counts().head(1) . student 196 Name: occupation, dtype: int64 . Passo 10. Qual a idade m&#233;dia dos usu&#225;rios? . df[&#39;age&#39;].mean() . 34.05196182396607 . Passo 11. Utilize o m&#233;todo describe() para obter diversas informa&#231;&#245;es a respeito do DataFrame. . users.describe() . user_id age . count 943.000000 | 943.000000 | . mean 472.000000 | 34.051962 | . std 272.364951 | 12.192740 | . min 1.000000 | 7.000000 | . 25% 236.500000 | 25.000000 | . 50% 472.000000 | 31.000000 | . 75% 707.500000 | 43.000000 | . max 943.000000 | 73.000000 | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; Exerc&#237;cios de manipula&#231;&#227;o de dados - Parte 2 . ! git clone https://github.com/Mario-RJunior/data-manipulation-exercises . Cloning into &#39;data-manipulation-exercises&#39;... remote: Enumerating objects: 31, done. remote: Total 31 (delta 0), reused 0 (delta 0), pack-reused 31 Unpacking objects: 100% (31/31), done. . cd data-manipulation-exercises/ . /content/data-manipulation-exercises/data-manipulation-exercises . ls . datasets/ Manipulacao_de_Dados_Ex_02.ipynb README.md Manipulacao_de_Dados_Ex_01.ipynb Manipulacao_de_Dados_Ex_03.ipynb . import pandas as pd . Tarefa 1. Importe o dataset e salve os dados em um dataframe . crime = pd.read_csv(&#39;datasets/US_Crime_Rates_1960_2014.csv&#39;) . Tarefa 2. Qual o tipo de dados em cada coluna? . crime.dtypes . Year int64 Population int64 Total int64 Violent int64 Property int64 Murder int64 Forcible_Rape int64 Robbery int64 Aggravated_assault int64 Burglary int64 Larceny_Theft int64 Vehicle_Theft int64 dtype: object . Tarefa 3. Converta o tipo de dado da coluna Year para o tipo datetime . print(crime[&#39;Year&#39;].head()) crime[&#39;Year&#39;] = pd.to_datetime(crime[&#39;Year&#39;], format=&#39;%M&#39;) crime[&#39;Year&#39;].dtypes . 0 1960-01-01 1 1961-01-01 2 1962-01-01 3 1963-01-01 4 1964-01-01 Name: Year, dtype: datetime64[ns] . dtype(&#39;&lt;M8[ns]&#39;) . Tarefa 4. Configure a coluna Year como index do DataFrame. . crime.set_index(&#39;Year&#39;, inplace=True) crime.head() . Population Total Violent Property Murder Forcible_Rape Robbery Aggravated_assault Burglary Larceny_Theft Vehicle_Theft . Year . 1960-01-01 179323175 | 3384200 | 288460 | 3095700 | 9110 | 17190 | 107840 | 154320 | 912100 | 1855400 | 328200 | . 1961-01-01 182992000 | 3488000 | 289390 | 3198600 | 8740 | 17220 | 106670 | 156760 | 949600 | 1913000 | 336000 | . 1962-01-01 185771000 | 3752200 | 301510 | 3450700 | 8530 | 17550 | 110860 | 164570 | 994300 | 2089600 | 366800 | . 1963-01-01 188483000 | 4109500 | 316970 | 3792500 | 8640 | 17650 | 116470 | 174210 | 1086400 | 2297800 | 408300 | . 1964-01-01 191141000 | 4564600 | 364220 | 4200400 | 9360 | 21420 | 130390 | 203050 | 1213200 | 2514400 | 472800 | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; Tarefa 5. Remova a coluna Total do DataFrame. . crime.drop(columns=&#39;Total&#39;, inplace=True) crime.columns . Index([&#39;Population&#39;, &#39;Violent&#39;, &#39;Property&#39;, &#39;Murder&#39;, &#39;Forcible_Rape&#39;, &#39;Robbery&#39;, &#39;Aggravated_assault&#39;, &#39;Burglary&#39;, &#39;Larceny_Theft&#39;, &#39;Vehicle_Theft&#39;], dtype=&#39;object&#39;) . Tarefa 6. Encontre o n&#250;mero de roubos de carro do ano de 1978. . crime[crime.index == &#39;1978&#39;][&#39;Vehicle_Theft&#39;] . Year 1978-01-01 1004100 Name: Vehicle_Theft, dtype: int64 . Tarefa 7. Retorne a linha do ano em que houve o maior n&#250;mero de assasinatos. . crime[crime[&#39;Murder&#39;] == crime[&#39;Murder&#39;].max()] . Population Violent Property Murder Forcible_Rape Robbery Aggravated_assault Burglary Larceny_Theft Vehicle_Theft . Year . 1991-01-01 252177000 | 1911770 | 12961100 | 24700 | 106590 | 687730 | 1092740 | 3157200 | 8142200 | 1661700 | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; Tarefa 8. Retorne o n&#250;mero de assassinatos do ano em que foi registrado o menor n&#250;mero de roubo de carros. . crime[crime[&#39;Vehicle_Theft&#39;] == crime[&#39;Vehicle_Theft&#39;].min()][&#39;Murder&#39;] . Year 1960-01-01 9110 Name: Murder, dtype: int64 . Exerc&#237;cios de manipula&#231;&#227;o de dados - Parte 3 . ! git clone https://github.com/Mario-RJunior/data-manipulation-exercises . Cloning into &#39;data-manipulation-exercises&#39;... remote: Enumerating objects: 31, done. remote: Total 31 (delta 0), reused 0 (delta 0), pack-reused 31 Unpacking objects: 100% (31/31), done. . ls . data-manipulation-exercises/ Manipulacao_de_Dados_Ex_02.ipynb datasets/ Manipulacao_de_Dados_Ex_03.ipynb Manipulacao_de_Dados_Ex_01.ipynb README.md . cd data-manipulation-exercises/ . /content/data-manipulation-exercises/data-manipulation-exercises/data-manipulation-exercises . ls . datasets/ Manipulacao_de_Dados_Ex_02.ipynb README.md Manipulacao_de_Dados_Ex_01.ipynb Manipulacao_de_Dados_Ex_03.ipynb . import pandas as pd . Tarefa 1. Importe o dataset e salve os dados em um dataframe . users = pd.read_csv(&#39;datasets/users_dataset.csv&#39;, sep=&#39;|&#39;) . Tarefa 2. Descubra qual a idade m&#233;dia por ocupa&#231;&#227;o. . users.groupby(&#39;occupation&#39;)[&#39;age&#39;].mean() . occupation administrator 38.746835 artist 31.392857 doctor 43.571429 educator 42.010526 engineer 36.388060 entertainment 29.222222 executive 38.718750 healthcare 41.562500 homemaker 32.571429 lawyer 36.750000 librarian 40.000000 marketing 37.615385 none 26.555556 other 34.523810 programmer 33.121212 retired 63.071429 salesman 35.666667 scientist 35.548387 student 22.081633 technician 33.148148 writer 36.311111 Name: age, dtype: float64 . Tarefa 3. Para cada ocupa&#231;&#227;o descubra a idade m&#237;nima e m&#225;xima. . users.groupby(&#39;occupation&#39;).agg({&#39;age&#39;: [&#39;min&#39;, &#39;max&#39;]}) . age . min max . occupation . administrator 21 | 70 | . artist 19 | 48 | . doctor 28 | 64 | . educator 23 | 63 | . engineer 22 | 70 | . entertainment 15 | 50 | . executive 22 | 69 | . healthcare 22 | 62 | . homemaker 20 | 50 | . lawyer 21 | 53 | . librarian 23 | 69 | . marketing 24 | 55 | . none 11 | 55 | . other 13 | 64 | . programmer 20 | 63 | . retired 51 | 73 | . salesman 18 | 66 | . scientist 23 | 55 | . student 7 | 42 | . technician 21 | 55 | . writer 18 | 60 | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; Tarefa 4. Para cada combina&#231;&#227;o de ocupa&#231;&#227;o e sexo, calcule a idade m&#233;dia. . users.groupby([&#39;occupation&#39;, &#39;gender&#39;])[&#39;age&#39;].mean() . occupation gender administrator F 40.638889 M 37.162791 artist F 30.307692 M 32.333333 doctor M 43.571429 educator F 39.115385 M 43.101449 engineer F 29.500000 M 36.600000 entertainment F 31.000000 M 29.000000 executive F 44.000000 M 38.172414 healthcare F 39.818182 M 45.400000 homemaker F 34.166667 M 23.000000 lawyer F 39.500000 M 36.200000 librarian F 40.000000 M 40.000000 marketing F 37.200000 M 37.875000 none F 36.500000 M 18.600000 other F 35.472222 M 34.028986 programmer F 32.166667 M 33.216667 retired F 70.000000 M 62.538462 salesman F 27.000000 M 38.555556 scientist F 28.333333 M 36.321429 student F 20.750000 M 22.669118 technician F 38.000000 M 32.961538 writer F 37.631579 M 35.346154 Name: age, dtype: float64 . Tarefa 5. Para cada ocupa&#231;&#227;o calcule a porcentagem de homens e mulheres. . df2 = users.groupby(&#39;occupation&#39;)[&#39;gender&#39;].value_counts() porcentagem = df2.groupby(level=0).apply(lambda x: 100 * x / float(x.sum())) porcentagem . occupation gender administrator M 54.430380 F 45.569620 artist M 53.571429 F 46.428571 doctor M 100.000000 educator M 72.631579 F 27.368421 engineer M 97.014925 F 2.985075 entertainment M 88.888889 F 11.111111 executive M 90.625000 F 9.375000 healthcare F 68.750000 M 31.250000 homemaker F 85.714286 M 14.285714 lawyer M 83.333333 F 16.666667 librarian F 56.862745 M 43.137255 marketing M 61.538462 F 38.461538 none M 55.555556 F 44.444444 other M 65.714286 F 34.285714 programmer M 90.909091 F 9.090909 retired M 92.857143 F 7.142857 salesman M 75.000000 F 25.000000 scientist M 90.322581 F 9.677419 student M 69.387755 F 30.612245 technician M 96.296296 F 3.703704 writer M 57.777778 F 42.222222 Name: gender, dtype: float64 .",
            "url": "https://vinicius-l-r-matos.github.io/-Repositorio-DS/fastpages/jupyter/data%20exploration/2022/01/30/_01_30_Exerc%C3%ADcios_Pandas.html",
            "relUrl": "/fastpages/jupyter/data%20exploration/2022/01/30/_01_30_Exerc%C3%ADcios_Pandas.html",
            "date": " • Jan 30, 2022"
        }
        
    
  
    
        ,"post11": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://vinicius-l-r-matos.github.io/-Repositorio-DS/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post12": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://vinicius-l-r-matos.github.io/-Repositorio-DS/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "Sobre Mim",
          "content": "Meu nome é Vinicius Matos. . Sou um profissional com foco no empreendedorismo e resolução de problemas. Tenho contato com dados na área da saúde desde 2006, por formação universitária. Desde 2016, faço uso disso para aprimorar meu desempenho com Business Intelligence. Faço uso de dados para aprimorar minha oratória e argumentação. . . Possuo formação e experiência técnica complementar pregressa para aplicação em vendas, consultas acadêmicas, oratória, argumentação, saúde ocupacional, reabilitação, medicina tradicional chinesa e programação. . Gosto de compreender melhor os cenários, criar esquemas e levantar hipóteses. A fim de demonstrar isso, criei este repositório, onde espero dividir um pouco desta minha paixão com todos. . Linguagens de programação preferidas: . SQL | Java Scrypt | Phython | PowerBi | . Minhas Mídias: . LinkedIn . | Medium . | . Este website foi possível graças ao fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://vinicius-l-r-matos.github.io/-Repositorio-DS/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://vinicius-l-r-matos.github.io/-Repositorio-DS/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}